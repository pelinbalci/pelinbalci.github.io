---
title: "Introduction to Deep Learning"
id: "dl"
category: "deep-learning"
tags: ["deep learning", "neural networks", "PyTorch", "CNN", "RNN", "attention", "ai-generated"]
related: ["ml", "python", "transformers"]
date: "2025-11-24"
description: "Comprehensive introduction to deep learning concepts including neural networks, CNNs, RNNs, attention mechanisms, and backpropagation with PyTorch examples"
---

# Introduction to Deep Learning

**This article is generated by Claude Opus 4.5**

## Overview

Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence "deep") to learn complex patterns from data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.

## What Makes Deep Learning Different?

While traditional machine learning requires manual feature engineering, deep learning automatically learns hierarchical representations from raw data. Each layer learns increasingly abstract features.

### Key Concepts

- **Neural Network**: A computational model inspired by biological neurons
- **Layers**: Building blocks that transform input data
- **Weights & Biases**: Learnable parameters that the network optimizes
- **Activation Functions**: Non-linear functions that enable learning complex patterns
- **Loss Function**: Measures how well the model's predictions match the targets
- **Backpropagation**: Algorithm for computing gradients to update weights

## Getting Started with PyTorch

PyTorch is a popular deep learning framework known for its flexibility and Pythonic design.

### Installation

```bash
pip install torch torchvision
```

### Basic Tensor Operations

```python
import torch
import torch.nn as nn
import torch.optim as optim

# Create tensors
x = torch.tensor([1.0, 2.0, 3.0])
y = torch.tensor([4.0, 5.0, 6.0])

# Basic operations
z = x + y           # Element-wise addition
dot = torch.dot(x, y)  # Dot product

# Create a matrix
matrix = torch.randn(3, 4)  # Random 3x4 matrix
print(f"Matrix shape: {matrix.shape}")

# GPU support (if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tensor_gpu = matrix.to(device)
```

## Neural Network Fundamentals

### The Perceptron (Single Neuron)

```python
import torch
import torch.nn as nn

class Perceptron(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.linear = nn.Linear(input_size, 1)
        self.activation = nn.Sigmoid()
    
    def forward(self, x):
        return self.activation(self.linear(x))

# Create and use perceptron
perceptron = Perceptron(input_size=3)
input_data = torch.tensor([1.0, 2.0, 3.0])
output = perceptron(input_data)
print(f"Output: {output.item():.4f}")
```

### Multi-Layer Perceptron (MLP)

```python
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )
    
    def forward(self, x):
        return self.network(x)

# Example: Classification with 10 input features, 64 hidden units, 3 classes
model = MLP(input_size=10, hidden_size=64, output_size=3)
sample_input = torch.randn(1, 10)  # Batch size 1
output = model(sample_input)
print(f"Output shape: {output.shape}")  # [1, 3]
```

## Backpropagation

Backpropagation is the algorithm used to compute gradients of the loss with respect to each weight. It applies the chain rule of calculus to efficiently propagate errors backward through the network.

### How Backpropagation Works

1. **Forward Pass**: Compute predictions by passing input through the network
2. **Compute Loss**: Calculate the difference between predictions and targets
3. **Backward Pass**: Compute gradients using the chain rule
4. **Update Weights**: Adjust weights in the direction that reduces loss

### Manual Backpropagation Example

```python
import torch

# Simple network: y = w2 * relu(w1 * x + b1) + b2
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)
target = torch.tensor([1.0])

# Initialize weights with gradient tracking
w1 = torch.randn(3, 4, requires_grad=True)
b1 = torch.zeros(4, requires_grad=True)
w2 = torch.randn(4, 1, requires_grad=True)
b2 = torch.zeros(1, requires_grad=True)

# Forward pass
hidden = torch.relu(x @ w1 + b1)
output = hidden @ w2 + b2

# Compute loss (MSE)
loss = ((output - target) ** 2).mean()
print(f"Loss: {loss.item():.4f}")

# Backward pass - computes all gradients
loss.backward()

# Gradients are now available
print(f"w1 gradient shape: {w1.grad.shape}")
print(f"w2 gradient shape: {w2.grad.shape}")
```

### Training Loop with PyTorch

```python
import torch
import torch.nn as nn
import torch.optim as optim

# Create model, loss function, and optimizer
model = MLP(input_size=10, hidden_size=64, output_size=3)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
def train_step(model, X_batch, y_batch):
    model.train()
    
    # Forward pass
    outputs = model(X_batch)
    loss = criterion(outputs, y_batch)
    
    # Backward pass
    optimizer.zero_grad()  # Clear previous gradients
    loss.backward()        # Compute gradients
    optimizer.step()       # Update weights
    
    return loss.item()

# Example training
X_train = torch.randn(100, 10)  # 100 samples
y_train = torch.randint(0, 3, (100,))  # 3 classes

for epoch in range(10):
    loss = train_step(model, X_train, y_train)
    print(f"Epoch {epoch+1}, Loss: {loss:.4f}")
```

## Convolutional Neural Networks (CNNs)

CNNs are designed for processing grid-like data such as images. They use convolutional layers to automatically learn spatial hierarchies of features.

### Key Components

- **Convolutional Layer**: Applies filters to detect local patterns
- **Pooling Layer**: Reduces spatial dimensions while retaining important features
- **Stride**: Step size when sliding the filter
- **Padding**: Adding zeros around input to control output size

### CNN Architecture in PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        
        # Convolutional layers
        self.conv1 = nn.Conv2d(
            in_channels=1,    # Grayscale image
            out_channels=32,  # 32 filters
            kernel_size=3,    # 3x3 filter
            padding=1         # Same padding
        )
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        
        # Pooling layer
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # Batch normalization
        self.bn1 = nn.BatchNorm2d(32)
        self.bn2 = nn.BatchNorm2d(64)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Fully connected layers
        self.fc1 = nn.Linear(128 * 3 * 3, 256)  # After 3 pooling ops: 28->14->7->3
        self.fc2 = nn.Linear(256, num_classes)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # Conv block 1: [B, 1, 28, 28] -> [B, 32, 14, 14]
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        
        # Conv block 2: [B, 32, 14, 14] -> [B, 64, 7, 7]
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        
        # Conv block 3: [B, 64, 7, 7] -> [B, 128, 3, 3]
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        
        # Flatten: [B, 128, 3, 3] -> [B, 128*3*3]
        x = x.view(x.size(0), -1)
        
        # Fully connected layers
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)
        
        return x

# Create model and test with random image
cnn = CNN(num_classes=10)
sample_image = torch.randn(1, 1, 28, 28)  # Batch=1, Channels=1, H=28, W=28
output = cnn(sample_image)
print(f"CNN output shape: {output.shape}")  # [1, 10]
```

### Understanding Convolutions

```python
# Visualize what a convolution does
import torch
import torch.nn as nn

# Create a simple edge detection filter
edge_filter = torch.tensor([
    [-1, -1, -1],
    [-1,  8, -1],
    [-1, -1, -1]
], dtype=torch.float32).view(1, 1, 3, 3)

# Apply to a sample image
conv = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)
conv.weight.data = edge_filter

sample = torch.randn(1, 1, 8, 8)
filtered = conv(sample)
print(f"Input shape: {sample.shape}, Output shape: {filtered.shape}")
```

## Recurrent Neural Networks (RNNs)

RNNs are designed for sequential data where the order matters (text, time series, audio). They maintain a hidden state that captures information from previous time steps.

### Basic RNN

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        
        # RNN layer
        self.rnn = nn.RNN(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True  # Input shape: [batch, seq_len, features]
        )
        
        # Output layer
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x, hidden=None):
        # x shape: [batch, seq_len, input_size]
        
        # Initialize hidden state if not provided
        if hidden is None:
            hidden = torch.zeros(1, x.size(0), self.hidden_size)
        
        # RNN forward pass
        output, hidden = self.rnn(x, hidden)
        # output shape: [batch, seq_len, hidden_size]
        # hidden shape: [num_layers, batch, hidden_size]
        
        # Take the last output for classification
        last_output = output[:, -1, :]  # [batch, hidden_size]
        prediction = self.fc(last_output)
        
        return prediction, hidden

# Example: Sequence classification
rnn = SimpleRNN(input_size=10, hidden_size=64, output_size=5)
sequence = torch.randn(32, 20, 10)  # Batch=32, SeqLen=20, Features=10
output, _ = rnn(sequence)
print(f"RNN output shape: {output.shape}")  # [32, 5]
```

### LSTM (Long Short-Term Memory)

LSTMs solve the vanishing gradient problem in vanilla RNNs using gates that control information flow.

```python
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes):
        super().__init__()
        
        # Embedding layer for text
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTM layer
        self.lstm = nn.LSTM(
            input_size=embed_dim,
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            dropout=0.3,
            bidirectional=True  # Process sequence both directions
        )
        
        # Output layer (bidirectional doubles hidden size)
        self.fc = nn.Linear(hidden_size * 2, num_classes)
    
    def forward(self, x):
        # x shape: [batch, seq_len] - token indices
        
        # Embed tokens
        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]
        
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(embedded)
        # lstm_out: [batch, seq_len, hidden_size*2]
        # hidden: [num_layers*2, batch, hidden_size]
        
        # Concatenate final hidden states from both directions
        hidden_cat = torch.cat([hidden[-2], hidden[-1]], dim=1)
        
        # Classification
        output = self.fc(hidden_cat)
        return output

# Example: Text classification
lstm_model = LSTMClassifier(vocab_size=10000, embed_dim=128, hidden_size=256, num_classes=3)
text_input = torch.randint(0, 10000, (16, 50))  # Batch=16, SeqLen=50
output = lstm_model(text_input)
print(f"LSTM output shape: {output.shape}")  # [16, 3]
```

### GRU (Gated Recurrent Unit)

GRUs are a simpler alternative to LSTMs with fewer parameters.

```python
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=2):
        super().__init__()
        
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        output, hidden = self.gru(x)
        # Use last time step output
        return self.fc(output[:, -1, :])

# Time series prediction
gru = GRUModel(input_size=1, hidden_size=64, output_size=1)
time_series = torch.randn(32, 100, 1)  # 100 time steps
prediction = gru(time_series)
print(f"GRU prediction shape: {prediction.shape}")  # [32, 1]
```

## Attention Mechanisms

Attention allows models to focus on relevant parts of the input when making predictions. It's the foundation of modern architectures like Transformers.

### Basic Attention

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Attention(nn.Module):
    """Simple additive attention mechanism."""
    
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.Linear(hidden_size, 1)
    
    def forward(self, encoder_outputs):
        # encoder_outputs: [batch, seq_len, hidden_size]
        
        # Compute attention scores
        scores = self.attention(encoder_outputs)  # [batch, seq_len, 1]
        scores = scores.squeeze(-1)  # [batch, seq_len]
        
        # Softmax to get attention weights
        weights = F.softmax(scores, dim=1)  # [batch, seq_len]
        
        # Weighted sum of encoder outputs
        context = torch.bmm(
            weights.unsqueeze(1),  # [batch, 1, seq_len]
            encoder_outputs         # [batch, seq_len, hidden_size]
        ).squeeze(1)  # [batch, hidden_size]
        
        return context, weights
```

### Scaled Dot-Product Attention

This is the attention mechanism used in Transformers.

```python
class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.scale = d_k ** 0.5
    
    def forward(self, query, key, value, mask=None):
        """
        Args:
            query: [batch, seq_len, d_k]
            key: [batch, seq_len, d_k]
            value: [batch, seq_len, d_v]
            mask: Optional mask for padding or causal attention
        """
        # Compute attention scores
        scores = torch.bmm(query, key.transpose(1, 2)) / self.scale
        # scores: [batch, seq_len, seq_len]
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        
        # Weighted sum of values
        output = torch.bmm(attention_weights, value)
        
        return output, attention_weights

# Example usage
d_k = 64
attention = ScaledDotProductAttention(d_k)

batch_size, seq_len = 8, 20
Q = torch.randn(batch_size, seq_len, d_k)
K = torch.randn(batch_size, seq_len, d_k)
V = torch.randn(batch_size, seq_len, d_k)

output, weights = attention(Q, K, V)
print(f"Attention output shape: {output.shape}")  # [8, 20, 64]
print(f"Attention weights shape: {weights.shape}")  # [8, 20, 20]
```

### Multi-Head Attention

Multi-head attention allows the model to attend to different representation subspaces.

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Linear projections for Q, K, V
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # Output projection
        self.W_o = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Linear projections and reshape for multi-head
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        # Q, K, V shape: [batch, num_heads, seq_len, d_k]
        
        # Reshape for batch matrix multiplication
        Q = Q.reshape(batch_size * self.num_heads, -1, self.d_k)
        K = K.reshape(batch_size * self.num_heads, -1, self.d_k)
        V = V.reshape(batch_size * self.num_heads, -1, self.d_k)
        
        # Apply attention
        attended, weights = self.attention(Q, K, V, mask)
        
        # Reshape back
        attended = attended.view(batch_size, self.num_heads, -1, self.d_k)
        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        # Final linear projection
        output = self.W_o(attended)
        
        return output

# Example
mha = MultiHeadAttention(d_model=512, num_heads=8)
x = torch.randn(4, 30, 512)  # Batch=4, SeqLen=30, Dim=512
output = mha(x, x, x)  # Self-attention
print(f"Multi-head attention output: {output.shape}")  # [4, 30, 512]
```

### Self-Attention with RNN

Combining attention with RNNs for sequence classification.

```python
class AttentionRNN(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        self.lstm = nn.LSTM(
            embed_dim, hidden_size,
            batch_first=True, bidirectional=True
        )
        
        self.attention = Attention(hidden_size * 2)
        self.fc = nn.Linear(hidden_size * 2, num_classes)
    
    def forward(self, x):
        # Embed
        embedded = self.embedding(x)
        
        # LSTM encoding
        lstm_out, _ = self.lstm(embedded)
        
        # Apply attention to get context vector
        context, attention_weights = self.attention(lstm_out)
        
        # Classify
        output = self.fc(context)
        
        return output, attention_weights

# Example
model = AttentionRNN(vocab_size=5000, embed_dim=128, hidden_size=128, num_classes=2)
text = torch.randint(0, 5000, (8, 50))
output, attn_weights = model(text)
print(f"Output: {output.shape}, Attention: {attn_weights.shape}")
```

## Complete Training Example

Here's a full example training a CNN on image classification:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Model
class ImageClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)
        )
        self.classifier = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

# Training function
def train(model, train_loader, epochs=10, device='cpu'):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            correct += predicted.eq(batch_y).sum().item()
            total += batch_y.size(0)
        
        scheduler.step()
        
        accuracy = 100. * correct / total
        print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f} - Accuracy: {accuracy:.2f}%")

# Create sample data and train
X = torch.randn(1000, 3, 32, 32)  # 1000 RGB images 32x32
y = torch.randint(0, 10, (1000,))  # 10 classes

dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

model = ImageClassifier()
train(model, loader, epochs=5)
```

## Activation Functions

```python
import torch
import torch.nn.functional as F

x = torch.linspace(-5, 5, 100)

# Common activation functions
relu = F.relu(x)           # max(0, x)
sigmoid = torch.sigmoid(x)  # 1 / (1 + e^-x)
tanh = torch.tanh(x)        # (e^x - e^-x) / (e^x + e^-x)
leaky_relu = F.leaky_relu(x, 0.01)  # x if x > 0 else 0.01*x
gelu = F.gelu(x)            # Used in Transformers

# In a model
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.GELU(),  # or nn.ReLU(), nn.LeakyReLU(), etc.
    nn.Linear(64, 10)
)
```

## Loss Functions

```python
# Classification
ce_loss = nn.CrossEntropyLoss()      # Multi-class
bce_loss = nn.BCEWithLogitsLoss()    # Binary

# Regression
mse_loss = nn.MSELoss()              # Mean Squared Error
mae_loss = nn.L1Loss()               # Mean Absolute Error
huber_loss = nn.SmoothL1Loss()       # Robust to outliers

# Sequence models
ctc_loss = nn.CTCLoss()              # Speech recognition

# Example usage
predictions = torch.randn(32, 10)    # 32 samples, 10 classes
targets = torch.randint(0, 10, (32,))
loss = ce_loss(predictions, targets)
```

## Key Takeaways

- Deep learning uses multiple layers to learn hierarchical representations
- Backpropagation efficiently computes gradients using the chain rule
- CNNs excel at spatial data (images) using convolutional filters
- RNNs/LSTMs/GRUs process sequential data with memory
- Attention mechanisms allow models to focus on relevant input parts
- PyTorch provides a flexible framework for building and training models
- Always use GPU acceleration when available for faster training

## Resources for Learning

### Online Courses
- [Deep Learning Specialization (Coursera)](https://www.coursera.org/specializations/deep-learning)
- [Fast.ai Practical Deep Learning](https://course.fast.ai/)
- [Stanford CS231n: CNNs for Visual Recognition](http://cs231n.stanford.edu/)
- [Stanford CS224n: NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)

### Books
- "Deep Learning" by Goodfellow, Bengio, and Courville
- "Dive into Deep Learning" (d2l.ai) - Free online book
- "Neural Networks and Deep Learning" by Michael Nielsen

### Documentation
- [PyTorch Documentation](https://pytorch.org/docs/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)

## Related Topics

- [Machine Learning Fundamentals](ml.html) - ML basics before diving into DL
- [Python Basics](python.html) - Programming foundation
- [Transformers](transformers.html) - Modern attention-based architectures

## Tools and Libraries

- **PyTorch**: Flexible deep learning framework
- **TensorFlow/Keras**: Alternative framework by Google
- **Hugging Face**: Pre-trained models and datasets
- **Weights & Biases**: Experiment tracking
- **TensorBoard**: Visualization toolkit

---

**Last Updated:** November 24, 2025  
**Difficulty Level:** Intermediate