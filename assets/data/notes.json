{
  "generated_at": "2026-02-02T19:27:19.958176Z",
  "nodes": [
    {
      "id": "optimization",
      "name": "Optimization Algorithms in Neural Networks",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "backpropagation",
        "cs231n"
      ],
      "date": "2020-10-14",
      "description": "Optimization Algorithms in Neural Network will be explained",
      "related": [
        "dl"
      ],
      "file": "2020-10-14-optimization_algorithms.md",
      "html": "<p>In this post, Optimization Algorithms in Neural Network will be explained. Thanks to Stanford University, the lecture \nvideos of cs231n can be found in YouTube and all materials are freely available (see the references). </p>\n<h1>First Order Optimization Algorithms</h1>\n<ul>\n<li>SGD</li>\n<li>SGD with Momentum</li>\n<li>SGD with Nesterov Momentum</li>\n<li>AdaGrad</li>\n<li>RMSProp</li>\n<li>Adam</li>\n</ul>\n<h2>SGD</h2>\n<pre><code>w = w - learningrate * gradient\n</code></pre>\n<ul>\n<li>Vanilla update. </li>\n<li>The simplest form of update is to change the parameters along the negative gradient direction </li>\n<li>Since the gradient indicates the direction of increase, but we usually wish to minimize a loss function</li>\n</ul>\n<h2>SGD with Momentum</h2>\n<pre><code>velocity = 0\nvelocity = momentum * velocity - learningrate * gradient\nw = w + velocity\n</code></pre>\n<ul>\n<li>With Momentum update, the parameter vector will build up velocity in any direction that has consistent gradient.</li>\n</ul>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/sgd_vs_momentum.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 1: SGD vs SGD with Momentum</div>\n</div>\n\n<h2>SGD with Nesterov Momentum</h2>\n<pre><code>old_velocity = velocity\nvelocity = momentum * velocity - learningrate * gradient\nw = w + (-momentum) * old_velocity + (1- (-momentum)) * velocity\n</code></pre>\n<ul>\n<li>https://arxiv.org/pdf/1212.0901v2.pdf : ADVANCES IN OPTIMIZING RECURRENT NETWORKS</li>\n<li>http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf : RNN </li>\n</ul>\n<h2>AdaGrad</h2>\n<p>Ref: https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf (-)\nRef: https://arxiv.org/pdf/1212.5701.pdf *</p>\n<pre><code>gradient_squared = 0 \ngradient_squared += gradient**2\nw += - learning_rate * gradient / (np.sqrt(gradient_squared) + eps)\n</code></pre>\n<p><strong>Pros:</strong> *</p>\n<p>While there is the hand tuned global learning rate, each dimension has its own dynamic rate. </p>\n<p>Since this dynamic rate grows with the inverse of the gradient magnitudes:\n    - large gradients have smaller learning rates\n    - small gradients have large learning rates. </p>\n<p>This is very beneficial for training deep neural networks since the scale of the gradients in each layer is often \ndifferent by several orders of magnitude, so the optimal learning rate should take that into account. </p>\n<p>Accumulation of gradient in the denominator has the same effects as annealing, reducing the learning rate over time. </p>\n<p><strong>Cons:</strong> * </p>\n<ul>\n<li>This method can be sensitive to initial conditions of the parameters and the corresponding gradients. </li>\n<li>If the initial gradients are large, the learning rates will be low for the remainder of training. </li>\n<li>This can be combatted by increasing the global learning rate, making the ADAGRAD method sensitive to the choice of \nlearning rate. </li>\n<li>Due to the continual accumulation of squared gradients in the denominator, the learning rate will continue to decrease \nthroughout training, eventually decreasing to zero and stopping training completely.</li>\n<li>Another exp:  A downside of Adagrad is that in case of Deep Learning, the monotonic learning rate usually proves too \naggressive and stops learning too early. **</li>\n</ul>\n<h2>RMS Prop</h2>\n<pre><code>gradient_squared = 0 \ngradient_squared = decay_rate * gradient_squared + (1 - decay_rate) * gradient**2\nw += - learning_rate * gradient / (np.sqrt(gradient_squared) + eps)\n</code></pre>\n<ul>\n<li>The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically \ndecreasing learning rate. </li>\n<li>In particular, it uses a moving average of squared gradients.</li>\n<li>\n<p>decay_rate is a hyperparameter and typical values are [0.9, 0.99, 0.999]. </p>\n</li>\n<li>\n<p>Notice that the w+= update is identical to Adagrad.</p>\n</li>\n<li>But the gradient_squared variable is a \u201cleaky\u201d. (has decay rate)</li>\n<li>Hence, RMSProp still modulates the learning rate of each weight based on the magnitudes of its gradients. \nIt has a beneficial equalizing effect.</li>\n<li>But unlike Adagrad, the updates do not get monotonically smaller.</li>\n</ul>\n<h1>Adam</h1>\n<p>Ref: https://arxiv.org/pdf/1412.6980.pdf</p>\n<pre><code># t is your iteration counter going from 1 to infinity\nfirst_moment, second_moment = 0, 0\nfor t in range(iterations):\n    first_moment = beta1 * first_moment + (1-beta1) * gradient\n    first_unbias = first_moment / (1-beta1**t)\n\n    second_moment = beta2 * second_moment + (1-beta2)*(gradient**2)\n    second_unbias = second_moment / (1-beta2**t)\n\n    w += - learning_rate * first_unbias / (np.sqrt(second_unbias) + eps)\n</code></pre>\n<p>The method computes individual adaptive learning rates for different parameters from estimates of first and second \nmoments of the gradients; the name Adam is derived from adaptive moment estimation.</p>\n<p>Without bias: </p>\n<p>At very first time step, second moment is zero. After one update teh second moment will still close to zero. When we\ndivide by second moment, we make a very large step at the beginning.  But this big step is not related with the geometry of the \nloss function, it is the nature of the formula. </p>\n<p>Bias correction term help to aviod the very large step at the beginning. </p>\n<p>You can start with these parameters:</p>\n<pre><code>beta1 = 0.9\nbeta2 = 0.999\nlr = 1e-3 or 1e-5\n</code></pre>\n<p>RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of first and second moment of the gradient. </p>\n<p>RMSProp also lacks a bias-correction term; this matters most in case of a value of beta2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence. </p>\n<p>Different Models comparison: {'sgd': 5e-3, 'sgd_momentum':  5e-3, 'rmsprop': 1e-4, 'adam': 1e-3}</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/different_models.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 2: Different models</div>\n</div>\n\n<p>Different Models comparison: {'sgd': 1e-3, 'sgd_momentum':  1e-3, 'rmsprop': 1e-3, 'adam': 1e-3}</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/different_models_same_lr.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 3: Different models with the same learning rate</div>\n</div>\n\n<h2>Change LR</h2>\n<p>you can change learning rate during the training:</p>\n<ul>\n<li>Step Decay</li>\n<li>Exponential Decay</li>\n<li>1/t decay</li>\n</ul>\n<p>Start with no decay. Select the best learning rate. Check the loss plot and decide where you need decay. </p>\n<h4>'sgd' :</h4>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/sgd_lr.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 4: SGD, learning rates: [1e-4, 5e-3, 3e-3, 1e-3, 1e-2]</div>\n</div>\n\n<h4>'sgd_momentum':</h4>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/sgd_moment_lr.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 5: SGD with Momentum, learning rates: [1e-4, 5e-3, 3e-3, 1e-3, 1e-2]</div>\n</div>\n\n<h4>'rmsprop':</h4>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/rms_lr.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 6: RMSProp, learning rates: [1e-4, 5e-4, 5e-3, 1e-3, 1e-2]</div>\n</div>\n\n<p>RMS Prop needs smaller lr: [1e-5, 5e-5, 5e-4, 1e-4]\n- running with  5e-05: (Epoch 5 / 5) train acc: 0.591000; val_acc: 0.359000\n- running with  0.0001: (Epoch 5 / 5) train acc: 0.557000; val_acc: 0.376000</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/rms_lr_smaller.png\" width=\"75%\">\n  <div class=\"figcaption\">RMSProp, smaller learning rates</div>\n</div>\n\n<h4>'adam':</h4>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/optimization_images/adam_lr.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 7: Adam, learning rates: [1e-4, 5e-4, 5e-3, 1e-3, 1e-2]</div>\n</div>\n\n<h2>Model Ensembles</h2>\n<p>Ref: https://www.youtube.com/watch?v=EK61htlw8hY&amp;ab_channel=TTIC</p>\n<p>One disadvantage of model ensembles is that they take longer to evaluate on test example. \nAn interested reader may find the recent work from Geoff Hinton on \u201cDark Knowledge\u201d inspiring, where the idea is to \n\u201cdistill\u201d a good ensemble back to a single model by incorporating the ensemble log likelihoods into a modified objective.</p>\n<p>Ref: https://www.youtube.com/watch?v=_JB0AO7QxSA&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=7&amp;ab_channel=StanfordUniversitySchoolofEngineering</p>\n<ol>\n<li>Train multiple independent models </li>\n<li>At test time average their results </li>\n</ol>\n<h1>Second Order Optimization Algorithms</h1>\n<ul>\n<li>https://arxiv.org/pdf/1311.2115.pdf (Quasi Newton)</li>\n<li>https://research.google/pubs/pub40565/ (Large Scale Distributed Deep Networks)</li>\n<li>https://en.wikipedia.org/wiki/Limited-memory_BFGS (LBFGS)</li>\n</ul>\n<h1>References</h1>\n<ul>\n<li>https://www.youtube.com/watch?v=_JB0AO7QxSA&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=7&amp;ab_channel=StanfordUniversitySchoolofEngineering</li>\n<li>https://cs231n.github.io/</li>\n</ul>\n<h1>Key Words</h1>\n<p>SGD, Momentum, Learning Rates, RMSProp, ADam, Optimization, Neural Networks</p>",
      "excerpt": "In this post, Optimization Algorithms in Neural Network will be explained. Thanks to Stanford University, the lecture videos of cs231n can be found in YouTube and all materials are freely available (s..."
    },
    {
      "id": "imageprocess",
      "name": "Image Processing",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "dl",
        "image"
      ],
      "date": "2020-10-19",
      "description": "Loading an image with PIL and processing it with PyTorch and Numpy",
      "related": [
        "dl"
      ],
      "file": "2020-10-19-image-operations.md",
      "html": "<p>In this notebook, I will explain how to load an image with PIL and explain some operations with PyTorch and Numpy. \nThese processes are part of Style Transfer in Convolutional Neural Networks, there are two images: content and style. \nThe next post defines how to transfer the style of one image to the content of the other image.</p>\n<p>This document is prepared by using Google Colab and the lecture notes from: https://classroom.udacity.com/courses/ud188</p>\n<p><strong>First of all; make sure that you are in the right content in drive.</strong></p>\n<pre><code class=\"language-python\">from google.colab import drive\ndrive.mount('/content/drive')\n</code></pre>\n<pre><code>Mounted at /content/drive\n</code></pre>\n<p>Use GPU if available</p>\n<pre><code class=\"language-python\"># move the model to GPU, if available\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\n</code></pre>\n<p>Import libraries</p>\n<pre><code class=\"language-python\"># import resources\n%matplotlib inline\n\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport requests\nfrom torchvision import transforms, models\n</code></pre>\n<p><strong>Load Image</strong></p>\n<p>The function below is useful for loading images. \n- img_path should be the path the image in your drive. \n- Since the large images slow down the process, max_size is defined as 400. \n- transforms from torchvision i used to crop, resize, normalize the image and we can turn it to tensor. </p>\n<p>Simply you can use following code: </p>\n<pre><code>image = Image.open(img_path).convert('RGB')\n</code></pre>\n<pre><code class=\"language-python\">def load_image(img_path, max_size=400, shape=None):\n    ''' Load in and transform an image, making sure the image\n       is &lt;= 400 pixels in the x-y dims.'''\n    if &quot;http&quot; in img_path:\n        response = requests.get(img_path)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(img_path).convert('RGB')\n\n    # large images will slow down processing\n    if max(image.size) &gt; max_size:\n        size = max_size\n    else:\n        size = max(image.size)\n\n    if shape is not None:\n        size = shape\n\n    in_transform = transforms.Compose([\n                        transforms.Resize(size),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.485, 0.456, 0.406), \n                                             (0.229, 0.224, 0.225))])\n\n    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n    image = in_transform(image)[:3,:,:].unsqueeze(0)\n\n    return image\n</code></pre>\n<p>Load images by file name</p>\n<pre><code class=\"language-python\">content = load_image('/content/drive/My Drive/Style_Transfer/images/octopus.jpg').to(device)\nstyle = load_image('/content/drive/My Drive/Style_Transfer/images/hockney.jpg').to(device)\n\nprint(content.shape)\nprint(style.shape)\n</code></pre>\n<pre><code>torch.Size([1, 3, 400, 592])\ntorch.Size([1, 3, 360, 263])\n</code></pre>\n<p>What does it look like? </p>\n<pre><code class=\"language-python\">content[0][0]\n</code></pre>\n<pre><code>tensor([[-1.8953, -1.9467, -1.9295,  ..., -0.1999, -0.1999, -0.3027],\n        [-1.9980, -2.0152, -1.9467,  ..., -0.4568, -0.3369, -0.1657],\n        [-1.9809, -1.9809, -1.9295,  ..., -0.8164, -0.5596, -0.3027],\n        ...,\n        [-1.9809, -1.9809, -1.9809,  ..., -1.0904, -1.1075, -1.1589],\n        [-1.9809, -1.9809, -1.9809,  ..., -1.0733, -1.1418, -1.2103],\n        [-1.9638, -1.9638, -1.9638,  ..., -1.0562, -1.1760, -1.2445]])\n</code></pre>\n<p><strong>Change the shape</strong></p>\n<p>The shape of two images are different, we can force the style image to be the same size as the content image. </p>\n<pre><code class=\"language-python\">content.shape[-2:]\n</code></pre>\n<pre><code>torch.Size([400, 592])\n</code></pre>\n<pre><code class=\"language-python\">style = load_image('/content/drive/My Drive/Style_Transfer/images/hockney.jpg', \n                   shape=content.shape[-2:]).to(device)\nprint(style.shape)\n</code></pre>\n<pre><code>torch.Size([1, 3, 400, 592])\n</code></pre>\n<p><strong>Display the image</strong></p>\n<pre><code class=\"language-python\"># helper function for un-normalizing an image \n# and converting it from a Tensor image to a NumPy image for display\ndef im_convert(tensor):\n    &quot;&quot;&quot; Display a tensor as an image. &quot;&quot;&quot;\n\n    image = tensor.to(&quot;cpu&quot;).clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1,2,0)\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n    image = image.clip(0, 1)\n\n    return image\n</code></pre>\n<pre><code class=\"language-python\"># display the images\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n# content and style ims side-by-side\nax1.imshow(im_convert(content))\nax2.imshow(im_convert(style))\n</code></pre>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/image_op/content_style.png\" width=\"50%\">\n  <div class=\"Figure 1: Content image(left) - Style image(right)\"> </div>\n</div>\n\n<p><strong>See the explanation for each step of the function:</strong></p>\n<p><strong>Copy content image &amp; detach</strong></p>\n<p>Ref: https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor</p>\n<p>torch.tensors are designed to be used in the context of gradient descent optimization, and therefore they hold not only a tensor with numeric values, but (and more importantly) the computational graph leading to these values. This computational graph is then used (using the chain rule of derivatives) to compute the derivative of the loss function w.r.t each of the independent variables used to compute the loss.</p>\n<p>As mentioned before, np.ndarray object does not have this extra \"computational graph\" layer and therefore, when converting a torch.tensor to np.ndarray you must explicitly remove the computational graph of the tensor using the detach() command.</p>\n<pre><code class=\"language-python\">image = content.to(&quot;cpu&quot;).clone().detach()\nprint(image.shape)\nprint(image.type)\n</code></pre>\n<pre><code>torch.Size([1, 3, 400, 592])\n&lt;built-in method type of Tensor object at 0x7ff3dcba6e10&gt;\n</code></pre>\n<p><strong>Turn to numpy</strong></p>\n<pre><code class=\"language-python\">image = image.numpy()\nprint(image.shape)\nprint(image.dtype)\n</code></pre>\n<pre><code>(1, 3, 400, 592)\nfloat32\n</code></pre>\n<p><strong>Use squeeze</strong></p>\n<p>numpy.squeeze() : Remove single-dimensional entries from the shape of an array.</p>\n<pre><code class=\"language-python\">image = image.squeeze()\nprint(image.shape)\n</code></pre>\n<pre><code>(3, 400, 592)\n</code></pre>\n<p><strong>Transpose</strong></p>\n<p>Transpose the dimension such that: Height, Width, Channel\n- Original: 3, 400, 592\n- Transpose(1,2,0) --&gt; 400, 592, 3 </p>\n<pre><code class=\"language-python\">image = image.transpose(1,2,0)\nimage.shape\n</code></pre>\n<pre><code>(400, 592, 3)\n</code></pre>\n<p><strong>Normalize</strong></p>\n<p>Ref: https://pytorch.org/docs/stable/torchvision/models.html</p>\n<p>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. </p>\n<p>You can use the following transform to normalize:</p>\n<pre><code>    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n</code></pre>\n<pre><code class=\"language-python\">image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\nimage.shape\n</code></pre>\n<pre><code>(400, 592, 3)\n</code></pre>\n<p><strong>Clip the values</strong></p>\n<p>Ref: https://numpy.org/doc/stable/reference/generated/numpy.clip.html</p>\n<p>Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.</p>\n<pre><code class=\"language-python\">image = image.clip(0, 1)\nimage\n</code></pre>\n<pre><code>array([[[0.05098039, 0.0509804 , 0.05882353],\n        [0.03921567, 0.03921569, 0.04705882],\n        [0.04313724, 0.04313725, 0.0509804 ],\n        ...,\n        [0.43921568, 0.43921569, 0.40000002],\n        [0.43921568, 0.43921569, 0.40000002],\n        [0.41568626, 0.41568628, 0.37647061]],\n\n       [[0.02745098, 0.02745099, 0.03529412],\n        [0.02352938, 0.02352943, 0.03137254],\n        [0.03921567, 0.03921569, 0.04313725],\n        ...,\n        [0.38039215, 0.37254903, 0.33725492],\n        [0.40784313, 0.40392158, 0.3647059 ],\n        [0.44705881, 0.43921569, 0.40392159]],\n\n       [[0.03137252, 0.03137255, 0.03529412],\n        [0.03137252, 0.03137255, 0.03529412],\n        [0.04313724, 0.04313725, 0.04313725],\n        ...,\n        [0.29803922, 0.28627453, 0.25098042],\n        [0.35686274, 0.34509805, 0.30980394],\n        [0.41568626, 0.40392158, 0.36862747]],\n\n       ...,\n\n       [[0.03137252, 0.02352943, 0.02745099],\n        [0.03137252, 0.02352943, 0.02745099],\n        [0.03137252, 0.02352943, 0.02745099],\n        ...,\n        [0.23529412, 0.21568628, 0.14117649],\n        [0.23137252, 0.21176472, 0.13333336],\n        [0.21960783, 0.20000002, 0.12156863]],\n\n       [[0.03137252, 0.02352943, 0.02745099],\n        [0.03137252, 0.02352943, 0.02745099],\n        [0.03137252, 0.02352943, 0.02745099],\n        ...,\n        [0.23921566, 0.21960786, 0.14117649],\n        [0.2235294 , 0.2039216 , 0.12549024],\n        [0.20784311, 0.18823531, 0.10980393]],\n\n       [[0.0352941 , 0.02745099, 0.03137254],\n        [0.0352941 , 0.02745099, 0.03137254],\n        [0.0352941 , 0.02745099, 0.03137254],\n        ...,\n        [0.24313724, 0.22352942, 0.14509807],\n        [0.21568625, 0.19607846, 0.11764705],\n        [0.19999996, 0.1803922 , 0.10196077]]])\n</code></pre>\n<p>Use matplotlib imshow to show the image</p>\n<pre><code class=\"language-python\">plt.imshow(image)\n</code></pre>\n<pre><code>&lt;matplotlib.image.AxesImage at 0x7ff3dae27cc0&gt;\n</code></pre>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/image_op/content.png\" width=\"50%\">\n  <div class=\"Content Image\"> </div>\n</div>\n\n<p><strong>Summary:</strong></p>\n<ul>\n<li>Load the image by using PIL</li>\n<li>Turn it to Tensor</li>\n<li>Change the shape</li>\n<li>Visualize the image<ul>\n<li>Why are we using constant terms when we normalize the image?</li>\n<li>What is squeeze?</li>\n<li>Transpose the 3 dimensional numpy array.</li>\n<li>How can we use clip?</li>\n</ul>\n</li>\n</ul>",
      "excerpt": "In this notebook, I will explain how to load an image with PIL and explain some operations with PyTorch and Numpy. These processes are part of Style Transfer in Convolutional Neural Networks, there ar..."
    },
    {
      "id": "styletransfer",
      "name": "Style Transfer",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "dl",
        "image"
      ],
      "date": "2020-10-20",
      "description": "How to transfer the style of an image to another image using CNN",
      "related": [
        "imageprocess"
      ],
      "file": "2020-10-20-style-transfer.md",
      "html": "<p>In this post, I would like to share my notes from the 'Style Transfer' lecture given by Udacity, Intro to Deep Learning \nwith PyTorch. You may find the references below. </p>\n<p>The key is using the technique that a trained CNN to separate the content from the style of an image. \nThen you can merge the content of one image and style of another and you will get a new image.  </p>\n<pre><code>content image + style image = target image\n</code></pre>\n<p><strong>Separating Style &amp; Content</strong></p>\n<p>In deep networks, the image is transformed and the layers care about the content of the image, rather than texture and \ncolors. But what about style? It is texture, colors, curvature...</p>\n<h2>VGG19 Paper Summary</h2>\n<p>In this paper[1]  style transfer uses the features found in 19 layer VGG network. </p>\n<ul>\n<li>Input: a color image.</li>\n<li>Pass through conv and pool layers. </li>\n<li>Finally 3 FC layers. Classify the image.</li>\n<li>In btw 5 pooling layers --&gt; 2 or 4 conv layers.</li>\n</ul>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/image_op/vgg_fig.png\" width=\"75%\">\n  <div class=\"figcaption\">Figure 1: VGG Layers for Style and Content Images</div>\n</div>\n\n<h3>Explanation of the Figure:</h3>\n<p>This is an image representations in a Convolutional Neural Network (CNN). </p>\n<p>A given input image is represented as a set of filtered images at each processing stage in the CNN. While the number of \ndifferent filters increases along the processing hierarchy, the size of the filtered\nimages is reduced by some down sampling mechanism (e.g. max-pooling) leading to a decrease in the total number of units \nper layer of the network. </p>\n<h4>Content Reconstructions:</h4>\n<p>We can visualise the information at different processing stages in the CNN by reconstructing the input image from only \nknowing the network\u2019s responses in a particular layer. </p>\n<p>We reconstruct the input image from from layers:\n- \u2018conv1 2\u2019 (a),\n- \u2018conv2 2\u2019 (b), \n- \u2018conv3 2\u2019 (c), \n- \u2018conv4 2\u2019 (d)\n- \u2018conv5 2\u2019 (e) of the original VGG-Network. </p>\n<p>We find that reconstruction from lower layers is almost perfect (a\u2013c). In higher layers of the network, detailed pixel \ninformation is lost while the high-level content of the image is preserved (d,e). </p>\n<h4>Style Reconstructions</h4>\n<p>On top of the original CNN activations we use a feature space that captures the texture information of an\ninput image. The style representation computes correlations between the different features in different layers of the CNN. \nWe reconstruct the style of the input image from a style representation built on different subsets of CNN layers \n- \u2018conv1 1\u2019 (a), \n- \u2018conv1 1\u2019 and \u2018conv2 1\u2019 (b), \n- \u2018conv1 1\u2019, \u2018conv2 1\u2019 and \u2018conv3 1\u2019 (c), \n- \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019 and \u2018conv4 1\u2019 (d), \n- \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019 and \u2018conv5 1\u2019 (e). </p>\n<p>This creates images that match the style of a given image on an increasing scale while discarding information of the\nglobal arrangement of the scene.\" [1]</p>\n<h2>Content</h2>\n<ul>\n<li>Conv_Layer output is the content representation of the input. </li>\n<li>Next when it sees the style image it will extract features from multiple layers that represent the style of the image. </li>\n</ul>\n<p>Target image can be started from blank canvas or copy of the content image. Target image's content should be close to content image and\nits style should be close to the style image. </p>\n<p>We need to compare content representation od content image and content representation of target image. </p>\n<p>How? </p>\n<p><strong>Content Loss</strong></p>\n<pre><code>Tc: target content rep, Cc: content content rep\nCONTENT LOSS = 1/2 * Sum (Tc - Cc)^2\n</code></pre>\n<p>The only value that change is Tc. We are not training CNN, but we are using backpropagation to min the loss function.</p>\n<h2>Style</h2>\n<p>We need to calculate the difference btw target image and style image. We\nfind the similarities btw features in multiple layers in the network. </p>\n<ul>\n<li>Include correlations btw multiple layers of different sizes</li>\n<li>we can obtain multiscale style representation of te input image. </li>\n</ul>\n<p><strong>Style representation calculation:</strong></p>\n<p>It is calculated by using all conv layers first channel: conv1_1, conv2_1 ...\nThe correlations at each layer are given by a GRAM MATRIX.</p>\n<p><strong>GRAM MATRIX</strong></p>\n<ul>\n<li>Input image 4*4</li>\n<li>Use 8 filters for conv layer</li>\n<li>Output: 4*4 in height and weight and 8 in depth. --&gt; It has 8 features maps that we want to find relationships between.</li>\n</ul>\n<p>Operations:\n- Vectorize the values in this layer: 8 * 4 * 4 -&gt; 4*4=16 (one feature map): <code>8*16</code>\n- Take the transpose of this matrix : <code>8*16</code>\n- Multiply these two matrices: <code>8*8</code></p>\n<p>Result of the gram matrix contains non localized information about the layer. </p>\n<p><strong>What is nonlocalized info??</strong></p>\n<p>It is information that would still be there even if an image was shuffled around in space. Even if the content of a filtered image\nis not identifable, you should be able to see the prominent colors and textures ie. the style!</p>\n<p>Matrix(8 * 16) * Matrix(16 * 8) = Gram Matrix(8 * 8) --&gt; its values indicates the similarities btw the layers.  </p>\n<ul>\n<li>Ref: https://en.wikipedia.org/wiki/Gramian_matrix</li>\n</ul>\n<p>An important application is to compute linear independence: a set of vectors are linearly independent if and only if the\nGram determinant (the determinant of the Gram matrix) is non-zero.</p>\n<ul>\n<li>Ref: https://stackoverflow.com/questions/55546873/how-do-i-flatten-a-tensor-in-pytorch</li>\n</ul>\n<p>How to reshape the tensor?</p>\n<pre><code>import torch\ninput = torch.rand(3, 8, 8)\nprint(input.shape) # torch.Size([3, 8, 8])\nprint(input.flatten(start_dim=1, end_dim=2).shape) # torch.Size([3, 64])\nreshape_input  = input.flatten(start_dim=1, end_dim=2)\nreshape_input.shape # torch.Size([3, 64])\n</code></pre>\n<p>How to reshape the tensor? --&gt; In the solution</p>\n<pre><code># get the batch_size, depth, height, and width of the Tensor\n_, d, h, w = tensor.size()\n\n# reshape so we're multiplying the features for each channel\ntensor = tensor.view(d, h * w)\n\n# calculate the gram matrix\ngram = torch.mm(tensor, tensor.t())\n</code></pre>\n<p><strong>Style Loss</strong></p>\n<ul>\n<li>Ss = Style image style</li>\n<li>Ts = Target image style</li>\n</ul>\n<p>Loss = a * Sum(wi (Tsi - Ssi)^2  --&gt; a is a constant that accounts for the number of values in each layer.</p>\n<h2>Loss</h2>\n<p>Total Loss =Loss_content + Loss_style</p>\n<p>By using backpropagation we change the target image to minimize the loss.</p>\n<p>Since these two losses can have very different values: (0.1 + 102) we need to treat them equally:</p>\n<p>Total Loss = &alpha; * Loss_content + &Beta; * Loss_style</p>\n<p><strong>Effect of Alpha &amp; Beta Ratio:</strong></p>\n<ul>\n<li>&alpha; / &Beta; = 1/10 ---&gt; more content + less style (we can see the shape pf house clearly)</li>\n<li>&alpha; / &Beta; = 1/10000 ---&gt; less content + more style (we loose the shape of house)</li>\n</ul>\n<p>The more small &alpha; / &Beta; ratio the more style you see!</p>\n<h2>Coding Tips</h2>\n<ul>\n<li>\n<p>Load VGG features, not classification part</p>\n<pre><code>vgg = models.vgg19(pretrained=True).features\n</code></pre>\n</li>\n<li>\n<p>Freeze all VGG parameters since we're only optimizing the target image</p>\n<pre><code>for param in vgg.parameters():\n    param.requires_grad_(False)\n</code></pre>\n</li>\n<li>\n<p>Features of Vgg19:</p>\n<pre><code>Sequential(\n</code></pre>\n<p>(0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (17): ReLU(inplace=True)\n  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (24): ReLU(inplace=True)\n  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (26): ReLU(inplace=True)\n  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (31): ReLU(inplace=True)\n  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (33): ReLU(inplace=True)\n  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (35): ReLU(inplace=True)\n  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)</p>\n</li>\n<li>\n<p>Classifier of Vgg19:</p>\n<pre><code>    Sequential(\n</code></pre>\n<p>(0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Dropout(p=0.5, inplace=False)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace=True)\n  (5): Dropout(p=0.5, inplace=False)\n  (6): Linear(in_features=4096, out_features=1000, bias=True)\n)</p>\n</li>\n<li>\n<p>How to use cuda?</p>\n<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg.to(device)\n</code></pre>\n</li>\n<li>\n<p>Load image with image path</p>\n<pre><code>image = Image.open(img_path).convert('RGB')\n</code></pre>\n</li>\n<li>\n<p>Use tranformation to resize normalize and turn to tensor:</p>\n<pre><code>transforms.Compose([\n                transforms.Resize(size),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), \n                                     (0.229, 0.224, 0.225))])\n\n# discard the transparent, alpha channel (that's the :3) and add the batch dimension\nimage = in_transform(image)[:3,:,:].unsqueeze(0)\n</code></pre>\n</li>\n<li>\n<p>Load both style and content image. They may have different shapes. Use content image hegiht and weight for style image.</p>\n<pre><code>style = load_image(path, shape=content.shape[-2:]).to(device)\n</code></pre>\n</li>\n<li>\n<p>Since we turn the image to tensor:</p>\n<pre><code>print(content)\n\ntensor([[[[-1.8953, -1.9467, -1.9295,  ..., -0.1999, -0.1999, -0.3027],\n  [-1.9980, -2.0152, -1.9467,  ..., -0.4568, -0.3369, -0.1657],\n  [-1.9809, -1.9809, -1.9295,  ..., -0.8164, -0.5596, -0.3027],\n  ...,\n</code></pre>\n</li>\n<li>\n<p>If you want to see the image you should be turn it to numpy array, transpose, normalize it and clip the values, \nthen use plt.imshow(image) to see the image. </p>\n<pre><code>image = tensor.to(\"cpu\").clone().detach()\nimage = image.numpy().squeeze()\nimage = image.transpose(1,2,0)\nimage = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\nimage = image.clip(0, 1)\n</code></pre>\n</li>\n<li>\n<p>Select the layers you would like to use: {'0': conv1_1, '3': conv2_1 ...}</p>\n</li>\n<li>\n<p>Take the input image.  pass through each layer, store the output of the layer. If this layer is one of out layers, \nstore the output in features dictionary. </p>\n<pre><code>vgg._modules.items()\nodict_items([('0', Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))), ('1', ReLU(inplace=True)),\n\nname, layer in vgg._modules.items():\n\nname = '0'\nlayer = Conv2d(3, 64, kernel_size=(3, 3)\nlayers['0']= conv1_1\n\noutput = layer(input image)\n\nfeatures = {layers['0']: [output..]}\nfeatures = {'conv1_1': [output..]}\n</code></pre>\n</li>\n<li>\n<p>You can calculate, target, content and style features. </p>\n</li>\n<li>\n<p>Calculate gram matrix</p>\n<p># get the batch_size, depth, height, and width of the Tensor\n   _, d, h, w = tensor.size()</p>\n<pre><code># reshape so we're multiplying the features for each channel\ntensor = tensor.view(d, h * w)\n\n# calculate the gram matrix\ngram = torch.mm(tensor, tensor.t())\n</code></pre>\n</li>\n<li>\n<p>Calculate the gram matrices for each layer of our style representation</p>\n</li>\n<li>\n<p>Create target image by cloning the content image and it require gradient</p>\n<pre><code>target = content.clone().requires_grad_(True).to(device)\n</code></pre>\n</li>\n<li>\n<p>Assign weights to selected style layers. Assign alpha and beta.</p>\n<pre><code> weighting earlier layers more will result in *larger* style artifacts\n</code></pre>\n</li>\n<li>\n<p>Use optimizer = optim.Adam([target], lr=0.003)</p>\n</li>\n<li>\n<p>Decide the number of iterations. It may be 2000 or 5000</p>\n</li>\n<li>\n<p>content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)</p>\n</li>\n<li>\n<p>For each layer in layers:</p>\n<ul>\n<li>get the features (output) of target and style</li>\n<li>calculate gram matrix</li>\n<li>layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)</li>\n<li>style_loss += layer_style_loss / (d * h * w)</li>\n</ul>\n</li>\n<li>\n<p>Total Loss: content_weight * content_loss + style_weight * style_loss</p>\n</li>\n<li>\n<p>Optimization part:</p>\n<pre><code>optimizer.zero_grad()\ntotal_loss.backward()\noptimizer.step()\n</code></pre>\n</li>\n</ul>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://classroom.udacity.com/courses/ud188/lessons/c1541fd7-e6ec-4177-a5b1-c06f1ce09dd8/concepts/e9180da9-001a-426e-974a-0ef1e87846dc\">Udacity_Lecture</a></li>\n<li><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\">Gatys_Image_Style_Transfer_CVPR_2016_paper</a> [1]</li>\n<li><a href=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/style-transfer/Style_Transfer_Solution.ipynb\">Code</a></li>\n</ul>\n<h2>Key Words</h2>\n<ul>\n<li>Pre-trained VGG19, Style Transfer, Gram Matrix, PyTorch, Style Transfer Loss</li>\n</ul>",
      "excerpt": "In this post, I would like to share my notes from the 'Style Transfer' lecture given by Udacity, Intro to Deep Learning with PyTorch. You may find the references below. The key is using the technique ..."
    },
    {
      "id": "arduino-edgeimpulse",
      "name": "Arduino Edge Impulse Set Up",
      "category": "edge-ml",
      "tags": [
        "edge machine learning",
        "arduino"
      ],
      "date": "2021-11-22",
      "description": "Set Arduino Tiny ML Kit up in Edge Impulse",
      "related": [
        "arduinosensor"
      ],
      "file": "2021-11-22-Arduino-Edge-Impulse.md",
      "html": "<p>I've bought Arduino Tiny Machine Learning Kit for my tiny machine learning projects. As a start, I'm taking the lecture: Introduction to Embedded Machine Learning from Coursera. To set the device up to Edge impulse you need to install Edge Impulse CLI and Arduino CLI. \nHere you can find some tips so that you don't deal with errors during the installation: \nRemember that those tips are valid for Windows OS. </p>\n<h3>1. Sign up to edge impulse and create a project.</h3>\n<h3>2. Install Edge Impulse CLI:</h3>\n<ul>\n<li><a href=\"https://docs.edgeimpulse.com/docs/cli-installation\">https://docs.edgeimpulse.com/docs/cli-installation</a></li>\n<li>Make sure that you <strong>download  Visual Studio Community (using the \"Desktop development with C++\" workload).</strong> </li>\n<li>Download the latest version of python. </li>\n<li>Install node.js from: https://nodejs.org/en/</li>\n<li><strong>Install the additional tools when installing NodeJS.</strong> </li>\n</ul>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_arduino_images/nodejs.PNG\" width=\"60%\">\n  <div class=\"figcaption\">Figure 1: Node.js download</div>\n</div>\n\n<ul>\n<li>\n<p>Check this website for additional configurations: <a href=\"https://github.com/nodejs/node-gyp#on-windows\">https://github.com/nodejs/node-gyp#on-windows</a></p>\n</li>\n<li>\n<p>Set msvs version</p>\n<p><code>$npm config set msvs_version 2017</code></p>\n</li>\n<li>\n<p>If you have multiple Python versions installed, <strong>you need to identify which Python version node-gyp should use</strong>: \n  [https://github.com/nodejs/node-gyp#on-windows]</p>\n<p><code>$npm config set python /path/to/executable/python</code></p>\n</li>\n<li>\n<p>Additional info can be found here: <a href=\"https://github.com/Microsoft/nodejs-guidelines/blob/master/windows-environment.md#compiling-native-addon-modules\">https://github.com/Microsoft/nodejs-guidelines</a></p>\n</li>\n<li>\n<p>Install CLI tools via: </p>\n<p><code>$npm install -g edge-impulse-cli --force</code></p>\n</li>\n</ul>\n<p>(Note that before python config and downloading visual studio community, I got plenty of errors during installation of CLI tools. )</p>\n<h3>3. Install Arduino CLI</h3>\n<ul>\n<li>Install Arduino CLI to your C:/ directory from <a href=\"https://arduino.github.io/arduino-cli/0.20/installation/\">https://arduino.github.io/arduino-cli/0.20/installation/</a>. Then add the Arduino \n  CLI installation path to your PATH environment variable. </li>\n<li>\n<p>Run the following commands:</p>\n<p><code>$ arduino-cli core list</code></p>\n<p><code>$ arduino-cli board list</code></p>\n</li>\n</ul>\n<p>For me, those commands were both empty :( I will explain the solution in a minute. </p>\n<h3>4. Update the firmware</h3>\n<ul>\n<li>You need to download the latest Edge Impulse firmware and unzip the file: \n  <a href=\"https://docs.edgeimpulse.com/docs/arduino-nano-33-ble-sense#2-update-the-firmware\">https://docs.edgeimpulse.com/</a></li>\n<li>Open flash_windows.bat to flash the firmware. </li>\n<li>Wait until it is completed. </li>\n</ul>\n<p><code>Here, I got this error: Unknown FQBN: platform arduino:mbed is not installed</code></p>\n<p>If you don't care about this error and jump to the last step (like me!) with the command </p>\n<pre><code>$edge-impulse-daemon\n</code></pre>\n<p>you will get timeout error.  :)</p>\n<p><code>Failed to get info off device Timeout when waiting for &gt;  (timeout: 5000) onConnected</code></p>\n<p>Let's turn back to Step 3. All these errors are coming from the arduino:mbed installation.\nI searched for the solution from the following websites:</p>\n<ul>\n<li><a href=\"https://forum.edgeimpulse.com/t/incorrect-fqbn-error-installing-ei-firmware-for-arduino-nano-33ble-sense/1372/12\">forum_egde_impulse_1</a></li>\n<li><a href=\"https://forum.edgeimpulse.com/t/arduino-cli-version/1332/6\">forum_egde_impulse_2</a></li>\n<li><a href=\"https://forum.edgeimpulse.com/t/issue-uploading-edge-impulse-firmware-for-nano/1683/2\">forum_egde_impulse_3</a></li>\n<li><a href=\"https://forum.arduino.cc/t/problem-uploading-to-new-board-arduino-nano-33-ble/685958\">forum_egde_impulse_4</a> </li>\n<li><a href=\"https://create.arduino.cc/projecthub/B45i/getting-started-with-arduino-cli-7652a5\">get_start_with_arduino_1</a></li>\n<li><a href=\"https://docs.arduino.cc/software/ide-v1/tutorials/getting-started/cores/arduino-mbed_nano\">get_start_with_arduino_1</a></li>\n</ul>\n<p>Here you can find what I did to solve this problem:\n- Download Arduino IDE. \n- Download: Tools/Board Manager/ Arduino Mbed OS Nano Boards</p>\n<p>I think it is not helpful since $arduino-cli core list is still empty!</p>\n<ul>\n<li>\n<p>Run the following command:</p>\n<p><code>$ arduino-cli core install arduino:mbed --&gt; This gets error.</code></p>\n</li>\n<li>\n<p>Close the firewall and run the command above again. It works! </p>\n</li>\n<li>\n<p>Don't forget to open the firewall. </p>\n<p><code>$ arduino-cli core list</code></p>\n<p><code>$ arduino-cli board list</code>  are not empty!</p>\n</li>\n</ul>\n<p>I updated the firmware without getting any errors (Step 4). </p>\n<p>Then, with the latest step <code>$edge-impulse-daemon</code>, I've achieved to load my device to edge impulse :)</p>\n<pre><code>`Edge Impulse serial daemon v1.13.16\nEndpoints:\n    Websocket: wss://remote-mgmt.edgeimpulse.com\n    API:       https://studio.edgeimpulse.com/v1\n    Ingestion: https://ingestion.edgeimpulse.com\n\n? Which device do you want to connect to? COM3 (Microsoft)\n[SER] Connecting to COM3\n[SER] Serial is connected, trying to read config...\n[SER] Retrieved configuration\n[SER] Device is running AT command version 1.6.0\n\n? To which project do you want to connect this device? Pelin / arduino_motion\nSetting upload host in device... OK\nConfiguring remote management settings... OK\nConfiguring API key in device... OK\nConfiguring HMAC key in device... OK\n[SER] Device is not connected to remote management API, will use daemon\n[WS ] Connecting to wss://remote-mgmt.edgeimpulse.com\n[WS ] Connected to wss://remote-mgmt.edgeimpulse.com\n? What name do you want to give this device? pelin's arduino\n[WS ] Device \"pelin's arduino\" is now connected to project \"arduino_motion\"\n[WS ] Go to https://studio.edgeimpulse.com/studio/62428/acquisition/training to build your machine learning model!`\n</code></pre>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_arduino_images/device.PNG\" width=\"90%\">\n  <div class=\"figcaption\">Figure 2: Finally! :) </div>\n</div>\n\n<h2>Key Words</h2>\n<p>Tiny ML, Arduino, Edge Impulse, Timeout, arduino:mbed</p>",
      "excerpt": "I've bought Arduino Tiny Machine Learning Kit for my tiny machine learning projects. As a start, I'm taking the lecture: Introduction to Embedded Machine Learning from Coursera. To set the device up t..."
    },
    {
      "id": "arduinodeploy",
      "name": "How to Deploy Model to Arduino?",
      "category": "edge-ml",
      "tags": [
        "edge machine learning",
        ""
      ],
      "date": "2021-11-28",
      "description": "Deployment of edge impulse to Arduino",
      "related": [
        "arduino-edgeimpulse"
      ],
      "file": "2021-11-28-Deploy-To-Arduino.md",
      "html": "<p>In this post, I will explain the deployment of edge impulse to Arduino. </p>\n<p>\"You can deploy your impulse to any device. This makes the model run without an internet connection, \nminimizes latency, and runs with minimal power consumption.\"</p>\n<p>Please check this website: <a href=\"https://docs.edgeimpulse.com/docs/running-your-impulse-arduino\">https://docs.edgeimpulse.com/docs/running-your-impulse-arduino</a></p>\n<ul>\n<li>Go to your project from Edge Impulse, click Deployment.</li>\n<li>Select Arduino from Deploy your impulse part. </li>\n<li>Select Arduino Nano 33 BLE Sense from Build Firmware.</li>\n<li>Select Quantized Version and click analyze. </li>\n<li>Build your model.</li>\n</ul>\n<p>It gives me arduino_motion-nano-33-ble-sense-v1.zip which includes flash.bat file. \nReboot arduino and run flash.bat.\nLunch command prompt run $edge-impulse-run --&gt; This starts live classification.</p>\n<p>But,  this is not what I want. I would like to <strong>download impulse library.</strong></p>\n<pre><code>I click build again without selecting Arduino Nano 33 BLE Sense from Build Firmware, \nit gives me ei-arduino-motion.zip this time.\n</code></pre>\n<ul>\n<li>Open Arduino IDE.</li>\n<li>Sketch --&gt; include licbrary --&gt; Add zip --&gt; select arduino-motion.zip</li>\n<li>Tools --&gt; board --&gt; Arduino Mbed OS Nano Boards --&gt; select Arduino Nano 33 BLE</li>\n<li>File --&gt; examples --&gt; Arduino Motion Inferencing --&gt; select Arduino Nano 33 BLE Sense Accelerometer</li>\n<li>Tools --&gt; Port --&gt; Select Arduino COM</li>\n<li>Click Upload  </li>\n</ul>\n<p>I encountered to this error:</p>\n<pre><code>Arduino_LSM9DS1.h: No such file or directory\n</code></pre>\n<p>Please check this website, I found it really heplpful: <a href=\"https://www.programmingelectronics.com/no-such-file-error/\">https://www.programmingelectronics.com/no-such-file-error/</a></p>\n<p>I installed  Arduino_LSM9DS1  from: </p>\n<pre><code>Open Arduino IDE: sketch --&gt; include library --&gt; manage libraries --&gt; Arduino_LSM9DS1\n</code></pre>\n<p>Upload completed with the errors below:</p>\n<pre><code>`C:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\ConvolutionFunctions\\arm_convolve_HWC_q7_RGB.c: In function 'arm_convolve_HWC_q7_RGB':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\ConvolutionFunctions\\arm_convolve_HWC_q7_RGB.c:125:25: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n                         *__SIMD32(pBuffer) = 0x0;\n                         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\ConvolutionFunctions\\arm_convolve_HWC_q7_RGB.c:158:25: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n                         *__SIMD32(pBuffer) = __PKHBT(bottom.word, top.word, 0);\n                         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_nn_mult_q15.c: In function 'arm_nn_mult_q15':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_nn_mult_q15.c:98:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pDst)++ = __PKHBT(out2, out1, 16);\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_nn_mult_q15.c:99:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pDst)++ = __PKHBT(out4, out3, 16);\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_nn_mult_q7.c: In function 'arm_nn_mult_q7':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_nn_mult_q7.c:80:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pDst)++ = __PACKq7(out1, out2, out3, out4);\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_q7_to_q15_reordered_no_shift.c: In function 'arm_q7_to_q15_reordered_no_shift':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_q7_to_q15_reordered_no_shift.c:106:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pDst)++ = in2;\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_q7_to_q15_reordered_no_shift.c:107:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pDst)++ = in1;\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\PoolingFunctions\\arm_pool_q7_HWC.c: In function 'compare_and_replace_if_larger_q7':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\PoolingFunctions\\arm_pool_q7_HWC.c:78:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pIn)++ = in.word;\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\PoolingFunctions\\arm_pool_q7_HWC.c: In function 'accumulate_q7_to_q15':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\PoolingFunctions\\arm_pool_q7_HWC.c:122:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pCnt)++ = __QADD16(vo1, in);\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\PoolingFunctions\\arm_pool_q7_HWC.c:125:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pCnt)++ = __QADD16(vo2, in);\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\dsp\\image\\processing.cpp: In function 'int ei::image::processing::yuv422_to_rgb888(unsigned char*, const unsigned char*, unsigned int, ei::image::processing::YUV_OPTIONS)':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\dsp\\image\\processing.cpp:73:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (unsigned int i = 0; i &lt; in_size_pixels; ++i) {\n                              ~~^~~~~~~~~~~~~~~~\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\tensorflow\\lite\\core\\api\\op_resolver.cpp: In function 'TfLiteStatus tflite::GetRegistrationFromOpCode(const tflite::OperatorCode*, const tflite::OpResolver&amp;, tflite::ErrorReporter*, const TfLiteRegistration**)':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\tensorflow\\lite\\core\\api\\op_resolver.cpp:34:20: warning: comparison is always false due to limited range of data type [-Wtype-limits]\n       builtin_code &lt; BuiltinOperator_MIN) {\n       ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n\nSketch uses 153392 bytes (15%) of program storage space. Maximum is 983040 bytes.\nGlobal variables use 48248 bytes (18%) of dynamic memory, leaving 213896 bytes for local variables. Maximum is 262144 bytes.\nDevice       : nRF52840-QIAA\nVersion      : Arduino Bootloader (SAM-BA extended) 2.0 [Arduino:IKXYZ]\nAddress      : 0x0\nPages        : 256\nPage Size    : 4096 bytes\nTotal Size   : 1024KB\nPlanes       : 1\nLock Regions : 0\nLocked       : none\nSecurity     : false\nErase flash\n\nDone in 0.000 seconds\nWrite 153400 bytes to flash (38 pages)\n[==============================] 100% (38/38 pages)\nDone in 6.438 seconds`\n</code></pre>\n<p>`</p>\n<p>Open Serial Monitor for Live Classification. \nDespite the errors, live classification is working well:)</p>\n<pre><code>Starting inferencing in 2 seconds...\nSampling...\nPredictions (DSP: 21 ms., Classification: 0 ms., Anomaly: 0 ms.): \n    circle: 0.00000\n    idle: 0.03516\n    left-right: 0.00000\n    up-down: 0.96484\n\nStarting inferencing in 2 seconds...\nSampling...\nPredictions (DSP: 20 ms., Classification: 0 ms., Anomaly: 0 ms.): \n    circle: 0.91406\n    idle: 0.00000\n    left-right: 0.08594\n    up-down: 0.00000\n\nStarting inferencing in 2 seconds...\nSampling...\nPredictions (DSP: 20 ms., Classification: 0 ms., Anomaly: 0 ms.): \n    circle: 0.00000\n    idle: 0.99219\n    left-right: 0.00781\n    up-down: 0.00000\n</code></pre>\n<p>Let's open the metadata:</p>\n<pre><code>C:\\Users\\pelin\\Documents\\Arduino\\libraries --&gt;  arduino_motion_inferencing --&gt; src --&gt; model_parameters\n</code></pre>\n<p>Open model_metadata.h.</p>\n<p>Here is the error: </p>\n<pre><code>#error \"Cannot use full TensorFlow Lite with EON\"\n</code></pre>\n<p>I installed  Arduino TensorflowLite via: </p>\n<pre><code>Open Arduino IDE: sketch --&gt; include library --&gt; manage libraries --&gt; Arduino TensorflowLite.\n</code></pre>\n<p>Start upload again. Same error :(</p>\n<pre><code>C:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\CMSIS\\NN\\Source\\PoolingFunctions\\arm_pool_q7_HWC.c:125:9: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\n         *__SIMD32(pCnt)++ = __QADD16(vo2, in);\n         ^\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\dsp\\image\\processing.cpp: In function 'int ei::image::processing::yuv422_to_rgb888(unsigned char*, const unsigned char*, unsigned int, ei::image::processing::YUV_OPTIONS)':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\dsp\\image\\processing.cpp:73:32: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (unsigned int i = 0; i &lt; in_size_pixels; ++i) {\n                              ~~^~~~~~~~~~~~~~~~\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\tensorflow\\lite\\core\\api\\op_resolver.cpp: In function 'TfLiteStatus tflite::GetRegistrationFromOpCode(const tflite::OperatorCode*, const tflite::OpResolver&amp;, tflite::ErrorReporter*, const TfLiteRegistration**)':\nC:\\Users\\pelin\\Documents\\Arduino\\libraries\\arduino_motion_inferencing\\src\\edge-impulse-sdk\\tensorflow\\lite\\core\\api\\op_resolver.cpp:34:20: warning: comparison is always false due to limited range of data type [-Wtype-limits]\n       builtin_code &lt; BuiltinOperator_MIN) {\n       ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n</code></pre>\n<p>Yet, when I open:</p>\n<pre><code>File -&gt; examples -&gt; arduino_motion_inferencing -&gt; Arduino Nano 33 BLE Sense Accelerometer Contionuous\n</code></pre>\n<p>Upload and open serial monitor: </p>\n<pre><code>Predictions (DSP: 28 ms., Classification: 0 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 1 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 0 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 0 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 1 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 1 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 1 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 1 ms., Anomaly: 0 ms.): idle  [ 0, 10, 0, 0, 0, 0, ]\nPredictions (DSP: 28 ms., Classification: 0 ms., Anomaly: 0 ms.): uncertain  [ 3, 0, 4, 0, 3, 0, ]\nPredictions (DSP: 26 ms., Classification: 0 ms., Anomaly: 0 ms.): uncertain  [ 3, 0, 4, 0, 3, 0, ]\nPredictions (DSP: 26 ms., Classification: 1 ms., Anomaly: 0 ms.): uncertain  [ 4, 0, 4, 0, 2, 0, ]\nPredictions (DSP: 28 ms., Classification: 0 ms., Anomaly: 0 ms.): uncertain  [ 5, 0, 4, 0, 1, 0, ]\nPredictions (DSP: 28 ms., Classification: 1 ms., Anomaly: 0 ms.): uncertain  [ 6, 0, 3, 0, 1, 0, ]\n....\nPredictions (DSP: 28 ms., Classification: 1 ms., Anomaly: 0 ms.): up-down  [1, 0, 1, 8, 0, 0, ]\n</code></pre>\n<p>No errors!</p>\n<p>First 4 numbers in the list represents:\n- 1: circle\n- 2: idle \n- 3: left_right\n- 4: up_down</p>\n<h2>Key Words</h2>\n<p>Tiny ML, Arduino, Edge Impulse, Deployment</p>",
      "excerpt": "In this post, I will explain the deployment of edge impulse to Arduino. \"You can deploy your impulse to any device. This makes the model run without an internet connection, minimizes latency, and runs..."
    },
    {
      "id": "fft",
      "name": "Fast Fourier Transform",
      "category": "edge-ml",
      "tags": [
        "edge machine learning",
        "fft",
        "audio"
      ],
      "date": "2021-01-01",
      "description": "A motion classifier project with Edge Impulse & Arduino",
      "related": [
        "edge-audio"
      ],
      "file": "2021-12-19-Fast_Fourier_Transform.md",
      "html": "<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/2021-12-19-Fast Fourier Tranform.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": ""
    },
    {
      "id": "edge-motion",
      "name": "Motion Classifier with Edge Impulse",
      "category": "edge-ml",
      "tags": [
        "edge machine learning",
        "arduino",
        "motion",
        "classification",
        "tensorflow"
      ],
      "date": "2021-01-01",
      "description": "A motion classifier project with Edge Impulse & Arduino",
      "related": [
        "arduinodeploy"
      ],
      "file": "2022-01-01-Motion-Classifier.md",
      "html": "<p>Hello! Happy new year everyone:) On the very first day of the year, I completed a motion classifier project with Edge Impulse &amp; Arduino. \nI would like to tell you the steps of the project, some problems I've faced with and how to solve them.</p>\n<p>I would like to thank Coursera and Edge Impulse to give us a chance to learn embedded machine learning. \nFind more about the lecture: <a href=\"https://www.coursera.org/learn/introduction-to-embedded-machine-learning\">Coursera</a></p>\n<h3>1. Connection</h3>\n<p>After creating a new project on Edge Impulse, I plugged in my Arduino board to my laptop, \nthen <strong>update the firmware</strong> ---&gt; this is very important do not skip this step. </p>\n<p>Then I open the command prompt and write:</p>\n<pre><code>edge-impulse-daemon --clean\n</code></pre>\n<p>I can connect my Arduino board with my new project in Edge Impulse. Go back to your project on the Edge Impulse website \nand check if the Arduino is connected. If you encountered with a problem, you can read my previous <a href=\"https://pelinbalci.com/tinyml/2021/11/22/Arduino-Edge-Impulse.html\">post</a></p>\n<h3>2. Collect Data</h3>\n<p>This is the funny part. You need to choose an item in your home that you can collect motion data. \nI wish to have a remote-controlled car :( Anyway, I decided to use my hairdryer and taped the board to the hairdryer \nas you can see below.</p>\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/hairdryer.jpeg\" width=\"40%\" height=\"60%\">\n  <div class=\"figcaption\">Setup </div>\n</div>\n\n<p>--</p>\n<p>Select Data- Acquisition, write the label name, and start sampling! There are 4 classes in my project, off, light, light_move, and heavy.</p>\n<pre><code>off: not working\nlight: working in mode 1\nlight_move: working in mode 1 and waving\nheavy: working in mode 2\n</code></pre>\n<p>Here is a short video of the collecting data process:</p>\n<p>-- </p>\n<video width=\"320\" height=\"240\" controls>\n  <source src=\"/assets/image_assets/tinyml_motion_images/datacollect.mp4\" type=\"video/mp4\">\n</video>\n\n<p>--</p>\n<p>I've collected 8 minutes of data (2 minutes for each class) and 2 minutes for test data.</p>\n<h3>3. Train &amp; Test Model</h3>\n<p>Impulse design is really easy thanks to Edge Impulse. First, you need to create impulse. \nYou will select processing block and learning block. The processing block will help you to generate \nfeatures, and the learning block is the classification model.</p>\n<p>-- </p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/impulse_design.PNG\" width=\"80%\">\n  <div class=\"figcaption\">Impulse Design </div>\n</div>\n\n<p>--</p>\n<p>You can check my previous post about <a href=\"https://pelinbalci.com/2021/12/19/Fast-Fourier-Tranform.html\">Fourier Transform</a>, \nIt is used for generating features, fortunately, the Edge Impulse handles the Fouirer Transform for us.</p>\n<p>After feature generation, I designed a neural network with 2 hidden layers and dropout layer with probability of 0.2 </p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Conv2D, Flatten, Reshape, MaxPooling1D, MaxPooling2D, BatchNormalization, TimeDistributed\nfrom tensorflow.keras.optimizers import Adam\n\n# model architecture\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu',\n    activity_regularizer=tf.keras.regularizers.l1(0.00001)))\nmodel.add(Dense(32, activation='relu',\n    activity_regularizer=tf.keras.regularizers.l1(0.00001)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(classes, activation='softmax', name='y_pred'))\n\n# this controls the learning rate\nopt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n# this controls the batch size, or you can manipulate the tf.data.Dataset objects yourself\nBATCH_SIZE = 32\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)\ncallbacks.append(BatchLoggerCallback(BATCH_SIZE, train_sample_count))\n\n# train the neural network\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel.fit(train_dataset, epochs=30, validation_data=validation_dataset, verbose=2, callbacks=callbacks)\n</code></pre>\n<p>Here is the validation score:</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/valid_acc.PNG\" width=\"70%\">\n  <div class=\"figcaption\">Validation Accuracy :) </div>\n</div>\n\n<p>--</p>\n<p>Then I choose model testing. The test score is 71.45%. I am happy with this result.</p>\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/test_acc.PNG\" width=\"70%\">\n  <div class=\"figcaption\">Test Accuracy :) </div>\n</div>\n\n<h3>4. Deployment</h3>\n<p>You can read my previous <a href=\"https://pelinbalci.com/tinyml/2021/11/28/Deploy-To-Arduino.html\">post</a> about deployment. Click Deployment, select Arduino, and the quantized version of the project (Please read more about Tensorflow Lite). Click build --&gt; this will give you a zip file.</p>\n<p>Now please close the terminal if it still opens. (I got \"port is busy\" error during uploading before I close the terminal)</p>\n<p>Open Arduino IDE.</p>\n<ul>\n<li>Sketch --&gt; Include library --&gt; Add Zip Library</li>\n<li>File --&gt; Examples --&gt; motion_classifier_inferencing --&gt; nano_ble33_sense_accelerometer --&gt; upload --&gt; this will take around 10 minutes.</li>\n</ul>\n<p>Then you can open Serial Monitor for live classification. Here is a short video for live classification. \nThere are some errors between the targets light and heavy but that's ok :)</p>\n<video width=\"320\" height=\"240\" controls>\n  <source src=\"/assets/image_assets/tinyml_motion_images/live_class.mp4\" type=\"video/mp4\">\n</video>\n\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/live_classification.PNG\" width=\"80%\">\n  <div class=\"figcaption\">Live Classification </div>\n</div>\n\n<h3>5. Improvement</h3>\n<p>I tried to increase the test accuracy for all classes. First I changed the model architecture:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Conv2D, Flatten, Reshape, MaxPooling1D, MaxPooling2D, BatchNormalization, TimeDistributed\nfrom tensorflow.keras.optimizers import Adam\n\n# model architecture\nmodel = Sequential()\nmodel.add(Dense(128, activation='relu',\n    activity_regularizer=tf.keras.regularizers.l1(0.00001)))\nmodel.add(Dense(64, activation='relu',\n    activity_regularizer=tf.keras.regularizers.l1(0.00001)))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(classes, activation='softmax', name='y_pred'))\n\n# this controls the learning rate\nopt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n# this controls the batch size, or you can manipulate the tf.data.Dataset objects yourself\nBATCH_SIZE = 32\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)\ncallbacks.append(BatchLoggerCallback(BATCH_SIZE, train_sample_count))\n\n# train the neural network\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel.fit(train_dataset, epochs=30, validation_data=validation_dataset, verbose=2, callbacks=callbacks)\n</code></pre>\n<p>Then, I've changed the parameters in Spectral features.</p>\n<p>Default filter value is low. Let's try \"heavy\" filter. The features separates the classes well but the test accuracy is worse than the low filter:</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/feature_heavy.png\" width=\"70%\">\n  <div class=\"figcaption\">Heavy Filter </div>\n</div>\n\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/test_heavy.png\" width=\"70%\">\n  <div class=\"figcaption\">Heavy Filter Test Accuracy </div>\n</div>\n\n<p>--</p>\n<p>Filter None gives very good result for test data :) </p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/feature_none.png\" width=\"70%\">\n  <div class=\"figcaption\">None Filter </div>\n</div>\n\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_motion_images/test_none.png\" width=\"70%\">\n  <div class=\"figcaption\">None Filter Test Accuracy </div>\n</div>\n\n<h3>Key Words</h3>\n<p>Tiny ML, Arduino, Edge Impulse, Motion, Classification</p>",
      "excerpt": "Hello! Happy new year everyone:) On the very first day of the year, I completed a motion classifier project with Edge Impulse & Arduino. I would like to tell you the steps of the project, some problem..."
    },
    {
      "id": "edge-audio",
      "name": "Audio Classifier with Edge Impulse",
      "category": "edge-ml",
      "tags": [
        "edge machine learning",
        "arduino",
        "audio",
        "classification",
        "tensorflow"
      ],
      "date": "2021-01-01",
      "description": "A motion classifier project with Edge Impulse & Arduino",
      "related": [
        "edge-motion"
      ],
      "file": "2022-01-17-Audio-Classifier.md",
      "html": "<p>In this post you can find my second project on Edge Impulse with Arduino Tiny ML kit. I used audio dataset this time. </p>\n<p>I would like to thank Coursera and Edge Impulse to give us a chance to learn embedded machine learning. \nFind more about the lecture: <a href=\"https://www.coursera.org/learn/introduction-to-embedded-machine-learning\">Coursera</a></p>\n<p>In the project description two audio datasets are given, faucet and noise. I added blender audio to this dataset. \nI've run the blender for 2 minutes. Then using Audacity interface, I divided the 2 minute-data to 50 1-second data. </p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_audio_images/audacity.png\" width=\"80%\">\n  <div class=\"figcaption\">Audacity Interface </div>\n</div>\n\n<p>--</p>\n<p>Rest of the steps are:</p>\n<ul>\n<li>Upload the data</li>\n<li>Create Impulse</li>\n<li>Generate Features</li>\n<li>Model Training</li>\n<li>Model Testing</li>\n</ul>\n<p>I've tried two different feature extraction method. I will explain the details of them later in the post. </p>\n<h3>Deployment to Arduino</h3>\n<p>On the deployment page, select Arduino and click build. ei-audio_classifier-arduino-1.0.3.zip file is downloaded.</p>\n<p>Since I didn't collect the data with Arduino, this is the first time I connected Arduino to the computer. </p>\n<p>Don't forget to update the firmware: run flash_windows. After the firmware is updated open the command prompt:</p>\n<pre><code>$edge-impulse-daemon --clean.\n</code></pre>\n<p>I reentered my credentials and selected my current project. </p>\n<p>Smile :)</p>\n<pre><code>[WS ] Device \"myarduino\" is now connected to project \"audio_classifier\"\n[WS ] Go to https://studio.edgeimpulse.com/studio/75736/acquisition/training to build your machine learning model!\n</code></pre>\n<p>Now it is time to use Arduino IDE. From Sketch -&gt; Add Zip Library, select ei-audio_classifier-arduino-1.0.3.zip</p>\n<p>Select File -&gt; Examples --&gt; audio-classifier-inferencing- nano_ble33_sense_microphone_continuous </p>\n<p>Here comes the final step: I clicked \"Upload\" to compile and send the program to Arduino. Unfortunately, I got this error:</p>\n<pre><code>Error sample buffer overrun. Decrease the number of slices per model window (EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW)\nERR: Failed to record audio...\n</code></pre>\n<p>I decreased the EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW from 3 to 2 by applying the directions on the website. \nRead more about this problem from: <a href=\"https://docs.edgeimpulse.com/docs/continuous-audio-sampling\">edge_impulse</a></p>\n<pre><code>/**\n * Define the number of slices per model window. E.g. a model window of 1000 ms\n * with slices per model window set to 4. Results in a slice size of 250 ms.\n * For more info: https://docs.edgeimpulse.com/docs/continuous-audio-sampling\n */\n# define EI_CLASSIFIER_SLICES_PER_MODEL_WINDOW 2\n</code></pre>\n<p>:)</p>\n<p>I run the blender, and the result is below :)\nThat's great!</p>\n<pre><code>Predictions (DSP: 165 ms., Classification: 253 ms., Anomaly: 0 ms.): \n    blender: 0.00000\n    faucet: 0.00000\n    noise: 0.99609\nPredictions (DSP: 165 ms., Classification: 253 ms., Anomaly: 0 ms.): \n    blender: 0.00000\n    faucet: 0.00000\n    noise: 0.99609\nPredictions (DSP: 165 ms., Classification: 253 ms., Anomaly: 0 ms.): \n    blender: 0.62695\n    faucet: 0.00000\n    noise: 0.37305\nPredictions (DSP: 165 ms., Classification: 253 ms., Anomaly: 0 ms.): \n    blender: 0.94531\n    faucet: 0.00000\n    noise: 0.05469\n</code></pre>\n<h2>Feature Extraction</h2>\n<p>I got better results with MFE Feature Extraction. I used 2d conv net for MFE, while I used 1DConv for MFFC.</p>\n<p>MFCCs are commonly derived as follows:\n- Take the Fourier transform of (a windowed excerpt of) a signal.\n- Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows or alternatively, cosine overlapping windows.\n- Take the logs of the powers at each of the mel frequencies. \n- Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n- The MFCCs are the amplitudes of the resulting spectrum. <a href=\"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\">wikipedia</a></p>\n<p>MFE:</p>\n<p>The Mel-scale is a perceptual scale of pitches judged by listeners to be equal in distance from one another. \nThe idea is to extract more features (more filter banks) in the lower frequencies, \nand less in the high frequencies, thus it performs well on sounds that can be distinguished by human ear. <a href=\"https://docs.edgeimpulse.com/docs/audio-mfe\">edge_impulse</a></p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_audio_images/mfcc_mfe_1.PNG\">\n  <div class=\"figcaption\">DSP </div>\n</div>\n\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_audio_images/mfcc_mfe_2.PNG\">\n  <div class=\"figcaption\">Parameters </div>\n</div>\n\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_audio_images/mfcc_mfe_3.PNG\" width=\"80%\">\n  <div class=\"figcaption\">MFE Separates better </div>\n</div>\n\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_audio_images/mfcc_mfe_4.PNG\">\n  <div class=\"figcaption\">Training Accuracy </div>\n</div>\n\n<p>--</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/tinyml_audio_images/mfcc_mfe_4.PNG\">\n  <div class=\"figcaption\">Test Accuracy </div>\n</div>\n\n<h3>Key Words</h3>\n<p>Tiny ML, Arduino, Edge Impulse, Audio, Classification</p>",
      "excerpt": "In this post you can find my second project on Edge Impulse with Arduino Tiny ML kit. I used audio dataset this time. I would like to thank Coursera and Edge Impulse to give us a chance to learn embed..."
    },
    {
      "id": "anomaly_numpy",
      "name": "Anomaly Detection",
      "category": "machine-learning",
      "tags": [
        "anomaly",
        "numpy"
      ],
      "date": "2022-11-05",
      "description": "Notebook for anomaly detection with NumPy",
      "related": [
        "ml"
      ],
      "file": "2022-11-05-Anomaly_Detection_with_Numpy.md",
      "html": "<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/2022-11-05-Anomaly_Detection_with_Numpy.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": ""
    },
    {
      "id": "wordvector",
      "name": "Word Vectors",
      "category": "genai",
      "tags": [
        "vectors"
      ],
      "date": "2023-01-01",
      "description": "Notebook for word vectors",
      "related": [
        "dl",
        "llm"
      ],
      "file": "2023-01-01-word_vectors.md",
      "html": "<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/2023-01-01-Word_Vectors.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": ""
    },
    {
      "id": "chart",
      "name": "Line Chart with Plotly",
      "category": "visualization",
      "tags": [
        "python",
        "visualization"
      ],
      "date": "2025-11-08",
      "description": "How to make scatter plots in Python with Plotly.",
      "related": [
        "python"
      ],
      "file": "2023-01-29-Line_Chart_with_Plotly.md",
      "html": "<p>In this file I will explain how to use plotly with markers and annotation. It is very easy to plot chart in plotly express!:)</p>\n<p>Let's create two datasets: they include sales &amp; anomalies for the first 10 days of Jan 2023. </p>\n<pre><code class=\"language-python\"># create dataset\nimport pandas as pd\ndf = pd.DataFrame({&quot;Date&quot;: [&quot;2023-01-01&quot;, &quot;2023-01-02&quot;, &quot;2023-01-03&quot;, &quot;2023-01-04&quot;, &quot;2023-01-05&quot;, &quot;2023-01-06&quot;, &quot;2023-01-07&quot;, \n              &quot;2023-01-08&quot;, &quot;2023-01-09&quot;, &quot;2023-01-10&quot;],\n     &quot;Analysis&quot;: [&quot;Normal&quot;, &quot;Anomaly&quot;, &quot;Normal&quot;, &quot;Anomaly&quot;, &quot;Anomaly&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;],\n     &quot;Sales&quot;: [120, 30, 115, 10, 5, 100, 99, 123, 134, 96]})\n\n</code></pre>\n<p>We need to convert the \"date\" to datetime to show it in x axis properly. </p>\n<pre><code># to datetime\ndf.Date = pd.to_datetime(df.Date)\ndf_with_group.Date = pd.to_datetime(df_with_group.Date)\n</code></pre>\n<p>Here is a simple way to plot a animation chart!</p>\n<pre><code class=\"language-python\">import plotly.express as px\nfig = px.line(df, x=&quot;Date&quot;, y=&quot;Sales&quot;, title='Daily Sales')\nfig.show()\n</code></pre>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/plotly_simple.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>\n\n<p>Now it is time to mark the anomalies! We will use Graph objects for this. Start with mark the anomaly dates:</p>\n<pre><code class=\"language-python\">anomaly_date = df[df.Analysis == &quot;Anomaly&quot;]\n</code></pre>\n<p>It is required to import graph objects to mark \"anomalies\".</p>\n<pre><code class=\"language-python\">import plotly.graph_objects as go\nfig = px.line(df, x=&quot;Date&quot;, y=&quot;Sales&quot;, title='Daily Sales')\nfig.add_traces(go.Scatter(x=anomaly_date[&quot;Date&quot;], y=anomaly_date[&quot;Sales&quot;], mode=&quot;markers&quot;, name=&quot;Anomaly&quot;, \n                          hoverinfo=&quot;skip&quot;))\nfig.show(renderer='notebook')\n</code></pre>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/plotly_mark.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>\n\n<p>If you want to see the \"Anomaly\" as a string on the chart, you should use \"add_annotation\":</p>\n<pre><code class=\"language-python\">fig = px.line(df, x=&quot;Date&quot;, y=&quot;Sales&quot;, title='Daily Sales')\nfig.add_traces(go.Scatter(x=anomaly_date[&quot;Date&quot;], y=anomaly_date[&quot;Sales&quot;], mode=&quot;markers&quot;, name=&quot;Anomaly&quot;, \n                          hoverinfo=&quot;skip&quot;))\nfor idx in range(len(anomaly_date)):\n     fig.add_annotation(dict(font=dict(color='rgba(0,0,200,0.8)',size=12),\n                                        x=anomaly_date[&quot;Date&quot;].iloc[idx],\n                                        y=anomaly_date[&quot;Sales&quot;].iloc[idx],\n                                        showarrow=False,\n                                        text=anomaly_date[&quot;Analysis&quot;].iloc[idx],\n                                        textangle=0,\n                                        xanchor='auto',  #['auto', 'left', 'center', 'right']\n                                        xref=&quot;x&quot;,\n                                        yref=&quot;y&quot;))\nfig.show()\n</code></pre>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/plotly_mark_annotation.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>\n\n<h3>How to embed plotly chart to md file?</h3>\n<p>It is a bit tricky when you want to publish your plotly express plots as md file. I prefer\nto convert my jupyter notebooks to html and publish them directly. However, when it comes to publish plotly there are many erros \nin github deployment. For example: Liquid syntax error</p>\n<p>I used jupyter notebook to create the plots and save the charts. Following function is used for saving plotly express as html file:</p>\n<pre><code class=\"language-python\">import plotly.express as px\nimport plotly.io as pio\nfig = px.line(df, x=&quot;Date&quot;, y=&quot;Sales&quot;, title='Daily Sales')\nfig.show()\npio.write_html(fig, file='plotly_simple.html', auto_open=False)\n</code></pre>\n<p>Then you can add the html in .md file: </p>\n<pre><code>&lt;div&gt;\n&lt;iframe \n    src=\"https://www.pelinbalci.com/assets/components/plotly_mark_annotation.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n&gt;&lt;/iframe&gt;\n&lt;/div&gt;\n</code></pre>\n<p>Happy learning! :)</p>\n<h1>References</h1>\n<ul>\n<li>https://raw.githubusercontent.com/plotly/plotly.py/master/doc/python/line-and-scatter.md</li>\n<li>https://stackoverflow.com/questions/60513164/display-interactive-plotly-chart-html-file-on-github-pages</li>\n<li>https://stackoverflow.com/questions/64241461/plotly-how-to-add-markers-at-specific-points-in-plotly-line-graph-python-pan</li>\n<li>https://stackoverflow.com/questions/68731627/plotly-express-line-chart-mark-specific-points-and-retain-hover-data</li>\n<li>https://davistownsend.github.io/blog/PlotlyBloggingTutorial/</li>\n</ul>",
      "excerpt": "In this file I will explain how to use plotly with markers and annotation. It is very easy to plot chart in plotly express!:) Let's create two datasets: they include sales & anomalies for the first 10..."
    },
    {
      "id": "kmeans",
      "name": "K-means with Numpy",
      "category": "machine-learning",
      "tags": [
        "machine learning",
        "unsupervised learning",
        "numpy",
        "medium"
      ],
      "date": "2023-03-07",
      "description": "Explanation of K-means algorithm with Numpy",
      "related": [
        "ml"
      ],
      "file": "2023-03-07-K_means_with_numpy.md",
      "html": "<p>In this post, I will explain the K-means algorithm via Numpy. Continue reading by clicking <a href=\"https://medium.com/@balci.pelin/k-means-with-numpy-3c207398c4d4\">here</a></p>",
      "excerpt": "In this post, I will explain the K-means algorithm via Numpy. Continue reading by clicking [here](https://medium.com/@balci.pelin/k-means-with-numpy-3c207398c4d4)"
    },
    {
      "id": "easyocr",
      "name": "EasyOCR in HuggingFace",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "dl",
        "image",
        "medium",
        "huggingface"
      ],
      "date": "2023-03-08",
      "description": "Create a space about EasyOCR in HuggingFace",
      "related": [
        "imageprocess"
      ],
      "file": "2023-03-08-EasyOCR_HF_Streamlit.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/easyocr-with-hugging-face-with-streamlit-f5a27666ec5c\">TL;DR? Read the full version on Medium</a> </p>\n<p>You can start creating your apps on Hugging Face platform today!</p>",
      "excerpt": "[TL;DR? Read the full version on Medium](https://medium.com/@balci.pelin/easyocr-with-hugging-face-with-streamlit-f5a27666ec5c) You can start creating your apps on Hugging Face platform today!"
    },
    {
      "id": "crossentropy",
      "name": "Cross-Entropy Loss",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "dl",
        "loss",
        "cross entropy",
        "medium"
      ],
      "date": "2023-03-11",
      "description": "Simple example for cross entropy loss",
      "related": [
        "dl"
      ],
      "file": "2023-03-11-Cross_Entropy_Loss.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/cross-entropy-loss-5568329c9d3\">Read the full version on Medium</a> </p>\n<p>I will explain the cross-entropy loss with a very simple example.</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/cross-entropy-loss-5568329c9d3) I will explain the cross-entropy loss with a very simple example."
    },
    {
      "id": "llm",
      "name": "LLM Introduction",
      "category": "genai",
      "tags": [
        "llm",
        "medium"
      ],
      "date": "2023-07-17",
      "description": "LLM Introduction",
      "related": [],
      "file": "2023-07-17-LLM_Intro.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/llm-introduction-7ededa51b78b\">Read the full version on Medium</a></p>\n<p>If you think that, now that ChatGPT handles everything and you are late to learn it, this article is for you:)</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/llm-introduction-7ededa51b78b) If you think that, now that ChatGPT handles everything and you are late to learn it, this article is fo..."
    },
    {
      "id": "configuration",
      "name": "LLM Configurations",
      "category": "genai",
      "tags": [
        "llm",
        "configuration",
        "top_k",
        "medium",
        "video"
      ],
      "date": "2023-07-22",
      "description": "A brief introduction to LLM configurations",
      "related": [
        "llm",
        "temperature"
      ],
      "file": "2023-07-22-LLM_Configurations.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/llm-inference-222c8e8a6ba7\">Read the full version on Medium</a> </p>\n<p>What are the configuration parameters that can influence the model\u2019s output during inference? </p>\n<p>Click here to watch the video on <a href=\"https://youtu.be/KbUPOJ8Fmzs\">YouTube</a></p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/llm-inference-222c8e8a6ba7) What are the configuration parameters that can influence the model\u2019s output during inference? Click here t..."
    },
    {
      "id": "fewshot",
      "name": "LLM FewShot Learning",
      "category": "genai",
      "tags": [
        "llm",
        "fewshot",
        "zeroshot",
        "prompt",
        "medium",
        "video"
      ],
      "date": "2023-07-24",
      "description": "Explanation of Zero-Shot One-Shot and Few Shot on code",
      "related": [
        "llm"
      ],
      "file": "2023-07-24-LLM_FewShot_Learning.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/llm-few-shot-learning-d7df1d2c4446\">Read the full version on Medium</a> </p>\n<p>Explanation of Zero-Shot One-Shot and Few Shot on code.</p>\n<p>Click here to watch the video on <a href=\"https://youtu.be/byRgzRCRSvw\">YouTube</a></p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/llm-few-shot-learning-d7df1d2c4446) Explanation of Zero-Shot One-Shot and Few Shot on code. Click here to watch the video on [YouTube]..."
    },
    {
      "id": "evaluation",
      "name": "LLM Evaluation",
      "category": "genai",
      "tags": [
        "evaluation",
        "llm",
        "medium"
      ],
      "date": "2023-08-01",
      "description": "A practical guide to measure the model performance",
      "related": [
        "llm"
      ],
      "file": "2023-08-01-LLM_Evaluation.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/llm-evaluation-6b63b3cfd48b\">TL;DR? Read the full version on Medium</a> </p>\n<p>This article is a practical guide to measure the model performance and to make sure that the model gives good results \nafter fine-tuning.</p>",
      "excerpt": "[TL;DR? Read the full version on Medium](https://medium.com/@balci.pelin/llm-evaluation-6b63b3cfd48b) This article is a practical guide to measure the model performance and to make sure that the model..."
    },
    {
      "id": "ner",
      "name": "NER",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "dl",
        "ner",
        "from scratch",
        "medium"
      ],
      "date": "2023-08-16",
      "description": "Build your NER data from scratch and learn the details of the NER model",
      "related": [
        "dl"
      ],
      "file": "2023-08-16-NER.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/ner-8eb9694ccad3\">Read the full version on Medium</a> </p>\n<p>Build your NER data from scratch and learn the details of the NER model.</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/ner-8eb9694ccad3) Build your NER data from scratch and learn the details of the NER model."
    },
    {
      "id": "finetuning",
      "name": "LLM Finetuning",
      "category": "genai",
      "tags": [
        "evaluation",
        "llm",
        "medium",
        "finetuning"
      ],
      "date": "2023-09-17",
      "description": "A practical guide to measure the model performance",
      "related": [
        "llm",
        "evaluation"
      ],
      "file": "2023-09-17-LLM_Finetuning.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/llm-finetuning-410e8a2738ef\">Read the full version on Medium</a> </p>\n<p>Finetuning doesn\u2019t have \nto be a mystery anymore! In this article, I\u2019ve created a simple notebook that breaks down the \nprocess in an easy-to-understand way.</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/llm-finetuning-410e8a2738ef) Finetuning doesn\u2019t have to be a mystery anymore! In this article, I\u2019ve created a simple notebook that bre..."
    },
    {
      "id": "faq",
      "name": "LLM FAQ",
      "category": "genai",
      "tags": [
        "evaluation",
        "llm",
        "prompt",
        "finetuning",
        "medium"
      ],
      "date": "2023-09-18",
      "description": "Q&A for LLM",
      "related": [
        "llm"
      ],
      "file": "2023-09-18-LLM_Q_A.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/llm-q-a-4c98031d9ea3\">Read the full version on Medium</a> </p>\n<p>If you\u2019ve been curious about LLMs and still have questions, this article \nis just for you. Get ready to dive deep into the world of language models!</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/llm-q-a-4c98031d9ea3) If you\u2019ve been curious about LLMs and still have questions, this article is just for you. Get ready to dive deep..."
    },
    {
      "id": "temperature",
      "name": "Temperature",
      "category": "genai",
      "tags": [
        "temperature",
        "llm",
        "video"
      ],
      "date": "2025-11-08",
      "description": "A brief description of temperature parameter",
      "related": [
        "configuration"
      ],
      "file": "2023-10-16-Temperature_parameter.md",
      "html": "<h1>Temperature</h1>\n<p><strong>Temperature</strong> is a parameter which is injected into the <strong>softmax function</strong>, enabling the users to manipulate the \noutput probabilities. It helps us to control the <strong>creativeness</strong> of a Large Language Model.</p>\n<p>The range of the temperature parameter is defined as 0 and 1 in OpenAI documentation. In the context of Cohere, \ntemperature values fall within the range of 0 to 5. <em>See the references below.</em></p>\n<hr />\n<p>This is the original softmax function: </p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/temperature_images/softmax.PNG\" width=\"50%\">\n  <div class=\"figcaption\"> </div>\n</div>\n\n<p>When we add Temperature parameter:</p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/temperature_images/softmax_temp.PNG\" width=\"50%\">\n  <div class=\"figcaption\"> </div>\n</div>\n\n<p>Remember that zj is the output of the neural network: it is a floating number. If you want to learn more about \nsoftmax function, read <a href=\"https://github.com/pelinbalci/Intro_Deep_Learning/blob/master/Intro_NN/notes/1_Perceptron_math.md#multiclass-classification--softmax:~:text=Multiclass%20Classification%20%26%20Softmax\">here</a>.</p>\n<hr />\n<ul>\n<li>As Temperature approaches 0, the output probabilities become more \"sharp\". One of the probability will be close to 1.</li>\n<li>As Temperature increases, the output probabilities become more \"flat\" or \"uniform\", reducing the difference between the probabilities of different elements.</li>\n</ul>\n<p>If we want repetitive answers, and no creativity at all, we can decrease the Temperature. If we want more creative answers, we can increase it.</p>\n<hr />\n<h1>Example</h1>\n<p>Let's imagine that our corpus has only 5 words: [\"donut\", \"cake\", \"apple\", \"juice\", \"book\"]</p>\n<p>The prediction of next token of given sentence: \"At the table, there is a delicious\" will be one of the words in the corpus. </p>\n<p>These are the original results: </p>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/temperature_images/softmax_df.PNG\" width=\"50%\">\n  <div class=\"figcaption\"> </div>\n</div>\n\n<p>You can try different temperature values to see how the output changes.</p>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/temperature_slider.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>\n\n<p>Listen my YouTube Vide from <a href=\"https://youtu.be/KbUPOJ8Fmzs\">here</a></p>\n<p>Happy Learning! :)</p>\n<h1>References</h1>\n<ul>\n<li>[1] https://platform.openai.com/docs/api-reference/audio/createTranscription#audio/createTranscription-temperature</li>\n<li>[2] https://txt.cohere.com/llm-parameters-best-outputs-language-ai/</li>\n<li>[3] https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/</li>\n</ul>",
      "excerpt": "# Temperature **Temperature** is a parameter which is injected into the **softmax function**, enabling the users to manipulate the output probabilities. It helps us to control the **creativeness** of ..."
    },
    {
      "id": "quantization",
      "name": "Quantization 1",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "dl",
        "quantization",
        "medium"
      ],
      "date": "2024-05-05",
      "description": "Unlocking the Power of Quantization: From Float32 to Int8",
      "related": [
        "finetuning",
        "dl"
      ],
      "file": "2024-05-05-Quantization_1.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/quantization-1-d05e5a61e0af\">Read the full version on Medium</a> </p>\n<p>Why do we need quantization?\n* Shrink models to a small size.\n* DL architectures are bigger and bigger.\n* A model can have 70 billion parameters.\n* NVIDIA T4 GPUs have 16 GB RAM.\n* Running models are still a challenge.\n* The aim is to get a smaller model.</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/quantization-1-d05e5a61e0af) Why do we need quantization? * Shrink models to a small size. * DL architectures are bigger and bigger. *..."
    },
    {
      "id": "quantization_2",
      "name": "Quantization 2",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "dl",
        "quantization",
        "medium"
      ],
      "date": "2024-05-06",
      "description": "Mastering Post-training Quantization: A Guide with PyTorch and TensorFlow",
      "related": [
        "quantization"
      ],
      "file": "2024-05-06-Quantization_2.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/quantization-2-7398a0ce7584\">Read the full version on Medium</a> </p>\n<p>Understanding quantization can feel like learning two different languages when comparing TensorFlow and PyTorch. In this \npost, I\u2019ll guide you through the differences and similarities, providing clear explanations along the way. You can find \nall the code references and notes in the provided <a href=\"https://github.com/pelinbalci/LLM_Notebooks/blob/main/Quantization.ipynb\">notebook</a></p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/quantization-2-7398a0ce7584) Understanding quantization can feel like learning two different languages when comparing TensorFlow and P..."
    },
    {
      "id": "poster",
      "name": "ECDP '24 Poster",
      "category": "conference",
      "tags": [
        "poster",
        "ner",
        "pathology"
      ],
      "date": "2024-06-05",
      "description": "Poster in 20th European Congress on Digital Pathology 2024",
      "related": [
        "devfest",
        "python"
      ],
      "file": "2024-06-05-ecdp_24_poster.md",
      "html": "<p>I'm excited to share that <a href=\"https://lnkd.in/dJ2WKKRF\">our poster</a> is in 20th European Congress on Digital Pathology 2024 | \nECDP2024! \u2728</p>\n<p>First and foremost, I owe my heartfelt thanks to hashtag#MemorialPathology Memorial Healthcare Group and Dr. Serdar \nBalc\u0131 for involving me in this project and for all their invaluable support and assistance throughout this journey.</p>\n<p>Our study highlights the importance of extracting information from pathology reports using Named Entity Recognition \n(NER). We conducted extensive tests utilizing various models (BERT Models and GPT) to ensure robust results.</p>\n<p>You can try our GPTs: \"PathText\" <a href=\"https://lnkd.in/dzyC6PJm\">here</a>. \u2728 And you may find a <a href=\"https://www.linkedin.com/posts/activity-7204810402281512960-_w5D?utm_source=share&amp;utm_medium=member_desktop\">video</a> \nabout that. It is currently in the beta testing phase, and we aim to develop it into a more comprehensive product.</p>",
      "excerpt": "I'm excited to share that [our poster](https://lnkd.in/dJ2WKKRF) is in 20th European Congress on Digital Pathology 2024 | ECDP2024! \u2728 First and foremost, I owe my heartfelt thanks to hashtag#MemorialP..."
    },
    {
      "id": "devfest",
      "name": "GDG Devfest'24 Notes",
      "category": "conference",
      "tags": [
        "llmops",
        "google",
        "devfest"
      ],
      "date": "2024-11-09",
      "description": "I've joined GDG devfest'24 event.",
      "related": [
        "reinvent"
      ],
      "file": "2024-11-09-devfest_notes.md",
      "html": "<p>I've joined GDG devfest'24 event yesterday. There were remarkable speakers. I would like to add my notes.\nYou may find the link of the conference <a href=\"https://devfest.istanbul/\">here</a></p>\n<h2>Devops MLOps LLMOps</h2>\n<p>I've combined the presentation notes and get a little help from chatgpt to prepare this chart. It shows the differences\nbetween Devops, MLOps and LLMOps Practices. The presentations are: </p>\n<ul>\n<li>From Devops to MLOps fro Majd Jamaah - Beyond Limits, GDE</li>\n<li>From Ideation to production: GENAI Application  Development and LLMOps from Emrah Mete - Microsoft</li>\n</ul>\n<div class=\"fig figcenter fighighlight\">\n  <img src=\"/assets/image_assets/devops_images/devops_mlops_llmops.png\" width=\"50%\">\n  <div class=\"figcaption\"> </div>\n</div>\n\n<h2>Useful Links</h2>\n<p>Google Trainings: https://cloud.google.com/learn/training/machinelearning-ai</p>\n<p>Alpha Proteo: https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/</p>\n<p>Nobel Prize: https://deepmind.google/discover/blog/demis-hassabis-john-jumper-awarded-nobel-prize-in-chemistry/</p>\n<p>Gemini Long Context Kaggle Competition: https://www.kaggle.com/competitions/gemini-long-context</p>\n<p>Agents:\n- https://haystack.deepset.ai/cookbook/web_enhanced_self_reflecting_agent?utm_campaign=developer-relations&amp;utm_source=devfest-ist-2024&amp;utm_medium=presentation\n- https://github.com/daronyondem/codesamples/tree/main/AutoGen\n- https://lilianweng.github.io/posts/2023-06-23-agent/</p>\n<p>Run Language Models: \n- https://ollama.com/\n- https://lmstudio.ai/\n- https://www.microsoft.com/en-us/research/blog/introducing-autogen-studio-a-low-code-interface-for-building-multi-agent-workflows/</p>",
      "excerpt": "I've joined GDG devfest'24 event yesterday. There were remarkable speakers. I would like to add my notes. You may find the link of the conference [here](https://devfest.istanbul/) ## Devops MLOps LLMO..."
    },
    {
      "id": "reinvent",
      "name": "AWS re:Invent Recap 1",
      "category": "conference",
      "tags": [
        "aws",
        "reinvent",
        "medium"
      ],
      "date": "2024-06-05",
      "description": "re:Invent notes",
      "related": [
        "reinvent_2"
      ],
      "file": "2024-12-17-aws-reinvent.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/aws-re-invent-2024-notes-part1-f185ed9b326b\">TL;DR? Read the full version on Medium</a> \nThis year, I was privileged to join the AWS re: Invent in Las Vegas! It was a wonderful week, I met incredible people \nfrom around the world, exchanging ideas, and learning from their unique experiences. I\u2019ve decided to write and share my notes :)</p>",
      "excerpt": "[TL;DR? Read the full version on Medium](https://medium.com/@balci.pelin/aws-re-invent-2024-notes-part1-f185ed9b326b) This year, I was privileged to join the AWS re: Invent in Las Vegas! It was a wond..."
    },
    {
      "id": "reinvent_2",
      "name": "AWS re:Invent Recap 2",
      "category": "conference",
      "tags": [
        "aws",
        "reinvent",
        "medium"
      ],
      "date": "2024-06-05",
      "description": "re:Invent notes",
      "related": [
        "reinvent"
      ],
      "file": "2024-12-18-aws-reinvent-2.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/aws-re-invent-2024-notes-part2-5318a2295bf9\">TL;DR? Read the full version on Medium</a> \nWelcome to the second part of my AWS re: Invent series! In this post, I\u2019ll be diving into some of the exciting announcements and sharing insights from the sessions I attended.</p>",
      "excerpt": "[TL;DR? Read the full version on Medium](https://medium.com/@balci.pelin/aws-re-invent-2024-notes-part2-5318a2295bf9) Welcome to the second part of my AWS re: Invent series! In this post, I\u2019ll be divi..."
    },
    {
      "id": "langchain_tools",
      "name": "Langchain Tools",
      "category": "genai",
      "tags": [
        "agents",
        "llm"
      ],
      "date": "2025-11-25",
      "description": "How LangChain calls the custom functions and how to leverage Tavily Search for internet searching.",
      "related": [
        "agentic-architecture"
      ],
      "file": "2025-01-18-LangChain_Tools.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/getting-started-with-langchain-tools-3beec9e1fb95\">Read the full version on Medium</a> </p>\n<p>Are you curious about AI agents and ready to build them from scratch? This post will be a great start for you. I will \nshow you how LangChain calls the custom functions and how to leverage Tavily Search for internet searching. In future \nposts, we will dive into how these constructs fit into the agentic architecture. Stay tuned to learn more about AI agents!\u2728\ud83c\udf89</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/getting-started-with-langchain-tools-3beec9e1fb95) Are you curious about AI agents and ready to build them from scratch? This post wil..."
    },
    {
      "id": "arduinosensor",
      "name": "Review of Arduino TinyML Kit Sensors",
      "category": "edge-ml",
      "tags": [
        "edge machine learning",
        "sensor",
        "arduino",
        "medium"
      ],
      "date": "2025-09-03",
      "description": "Getting Started with Tiny Machine Learning",
      "related": [
        "dl"
      ],
      "file": "2025-09-03-ArduinoTinyMLKitSensors.md",
      "html": "<p><a href=\"https://medium.com/@balci.pelin/review-of-arduino-tinyml-kit-sensors-b982f00518c7\">Read the full version on Medium</a></p>\n<p>Before jumping into machine learning, it\u2019s fun to simply play with the kit \u2014 blink an LED, read motion data, or snap a picture. In this post, we\u2019ll walk through some quick tests (Blink \u2192 IMU \u2192 Camera) that help you explore the hardware and get comfortable before machine learning applications.</p>",
      "excerpt": "[Read the full version on Medium](https://medium.com/@balci.pelin/review-of-arduino-tinyml-kit-sensors-b982f00518c7) Before jumping into machine learning, it\u2019s fun to simply play with the kit \u2014 blink ..."
    },
    {
      "id": "deepdream",
      "name": "Deep Dream Generator",
      "category": "deep-learning",
      "tags": [
        "deep dream",
        "image generation",
        "image",
        "inception",
        "CNN"
      ],
      "date": "2026-01-09",
      "description": "What happens when you ask a neural network to enhance what it sees, over and over again? You get Deep Dream\u2014one of the first techniques that let us peek inside the black box of AI",
      "related": [
        "imageprocess"
      ],
      "file": "2026-01-09-Deep_Dream_Generator.md",
      "html": "<p>I am reading the book How Smart Machines Think by Sean Gerrish. In one chapter, he explains the Deep Dream Generator. \nIt was very popular in 2015, and I used it a lot to make my pictures look dreamy. Thanks to Clade, I can now understand \nthe logic behind it. Here is a notebook that explains it better. </p>\n<p>The Colab link: https://github.com/pelinbalci/deep-dream/blob/main/deep_dream_tutorial.ipynb</p>\n<p>The actual deep dream generator: https://deepdreamgenerator.com/</p>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/deep_dream_tutorial.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": "I am reading the book How Smart Machines Think by Sean Gerrish. In one chapter, he explains the Deep Dream Generator. It was very popular in 2015, and I used it a lot to make my pictures look dreamy. ..."
    },
    {
      "id": "gnn_deep",
      "name": "GNN Architecture Deep Dive",
      "category": "deep-learning",
      "tags": [
        "gnn",
        "graph",
        "pytorch"
      ],
      "date": "2026-01-11",
      "description": "This notebook explains GNN architectures with input/output shapes at every layer.",
      "related": [
        "dl"
      ],
      "file": "2026-01-11-GNN_Deep_Dive.md",
      "html": "<p>\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working \nup to a functional implementation. Along the way, we'll see how GNNs differ from traditional matrix factorization and \nwhere they might shine.\"</p>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/1-GNN_Architecture_Deep_Dive.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": "\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working up to a functional implementation. Along the way, we'll see how GNNs differ from..."
    },
    {
      "id": "gnn_steps",
      "name": "GNN Step by Step",
      "category": "deep-learning",
      "tags": [
        "gnn",
        "graph",
        "pytorch"
      ],
      "date": "2026-01-12",
      "description": "This notebook shows exactly what happens inside a GCN layer using the actual MovieLens data",
      "related": [
        "gnn_deep"
      ],
      "file": "2026-01-12-GNN_Step_by_Step.md",
      "html": "<p>\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working \nup to a functional implementation. Along the way, we'll see how GNNs differ from traditional matrix factorization and \nwhere they might shine.\"</p>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/2-GCN_Step_by_Step.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": "\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working up to a functional implementation. Along the way, we'll see how GNNs differ from..."
    },
    {
      "id": "gnn_simple",
      "name": "GNN Simple",
      "category": "deep-learning",
      "tags": [
        "gnn",
        "graph",
        "pytorch"
      ],
      "date": "2026-01-13",
      "description": "This notebook is a learning journey from traditional collaborative filtering to Graph Neural Networks (GNNs) for recommendation systems.",
      "related": [
        "gnn_steps"
      ],
      "file": "2026-01-13-GNN_Simple.md",
      "html": "<p>\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working \nup to a functional implementation. Along the way, we'll see how GNNs differ from traditional matrix factorization and \nwhere they might shine.\"</p>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/3-GNN_Recommender_Simple.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": "\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working up to a functional implementation. Along the way, we'll see how GNNs differ from..."
    },
    {
      "id": "gnn_improved",
      "name": "GNN Improved",
      "category": "deep-learning",
      "tags": [
        "gnn",
        "graph",
        "pytorch"
      ],
      "date": "2026-01-14",
      "description": "This notebook shows the improvements on training.",
      "related": [
        "gnn_simple"
      ],
      "file": "2026-01-14-GNN_Improved.md",
      "html": "<p>\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working \nup to a functional implementation. Along the way, we'll see how GNNs differ from traditional matrix factorization and \nwhere they might shine.\"</p>\n<div>\n<iframe \n    src=\"https://www.pelinbalci.com/assets/components/4-GNN_Recommender_Improved_v3.html\" \n    width=\"100%\" \n    height=\"600\" \n    style=\"border:none;\"\n></iframe>\n</div>",
      "excerpt": "\"In this series, I'll build a graph neural network for movie recommendation, starting from the fundamentals and working up to a functional implementation. Along the way, we'll see how GNNs differ from..."
    },
    {
      "id": "agentic-architecture",
      "name": "Introduction to Agentic Architecture",
      "category": "genai",
      "tags": [
        "agents",
        "LLM",
        "autonomous systems",
        "AI architecture"
      ],
      "date": "2025-11-25",
      "description": "An overview of agentic AI systems, frameworks, and protocols that enable autonomous LLM-powered agents",
      "related": [
        "llm"
      ],
      "file": "agentic_architecture.md",
      "html": "<h1>Introduction to Agentic Architecture</h1>\n<p><strong>This article is generated by Claude Opus 4.5</strong></p>\n<h2>Overview</h2>\n<p>Agentic architecture represents a paradigm shift in how we build AI systems. Instead of simple prompt-response interactions, agentic systems can plan, reason, use tools, and execute multi-step tasks autonomously. These systems transform Large Language Models (LLMs) from passive responders into active problem-solvers.</p>\n<h2>What Makes an AI System \"Agentic\"?</h2>\n<p>An agentic system exhibits several key characteristics that distinguish it from traditional LLM applications:</p>\n<ul>\n<li><strong>Autonomy</strong>: The ability to make decisions and take actions without constant human intervention</li>\n<li><strong>Tool Use</strong>: Integration with external tools, APIs, and data sources to accomplish tasks</li>\n<li><strong>Planning</strong>: Breaking down complex goals into manageable steps</li>\n<li><strong>Memory</strong>: Maintaining context across interactions and learning from past experiences</li>\n<li><strong>Reflection</strong>: Self-evaluation and course correction when things go wrong</li>\n</ul>\n<h2>The Agent Loop</h2>\n<p>At its core, most agentic systems follow a similar pattern:</p>\n<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                         \u2502\n\u2502   User Goal/Task                        \u2502\n\u2502         \u2502                               \u2502\n\u2502         \u25bc                               \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502   \u2502  Observe  \u2502 \u25c4\u2500\u2500 Environment State   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502         \u2502                               \u2502\n\u2502         \u25bc                               \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502   \u2502   Think   \u2502 \u25c4\u2500\u2500 LLM Reasoning       \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502         \u2502                               \u2502\n\u2502         \u25bc                               \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502   \u2502    Act    \u2502 \u2500\u2500\u25ba Tool Execution      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502         \u2502                               \u2502\n\u2502         \u25bc                               \u2502\n\u2502   Goal Achieved? \u2500\u2500No\u2500\u2500\u25ba  Loop Back     \u2502\n\u2502         \u2502                               \u2502\n\u2502        Yes                              \u2502\n\u2502         \u2502                               \u2502\n\u2502         \u25bc                               \u2502\n\u2502   Return Result                         \u2502\n\u2502                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>\n<p>This \"observe-think-act\" loop allows agents to iteratively work toward goals, adapting their approach based on feedback from each action.</p>\n<h2>Key Concepts in Agentic Systems</h2>\n<h3>Tools and Function Calling</h3>\n<p>Tools extend an agent's capabilities beyond text generation. Modern LLMs support \"function calling\" where the model can request to execute specific functions with structured parameters:</p>\n<pre><code class=\"language-python\"># Example: Defining a tool for an agent\ntools = [\n    {\n        &quot;name&quot;: &quot;search_database&quot;,\n        &quot;description&quot;: &quot;Search the product database for items&quot;,\n        &quot;parameters&quot;: {\n            &quot;type&quot;: &quot;object&quot;,\n            &quot;properties&quot;: {\n                &quot;query&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Search query&quot;},\n                &quot;limit&quot;: {&quot;type&quot;: &quot;integer&quot;, &quot;description&quot;: &quot;Max results&quot;}\n            }\n        }\n    }\n]\n</code></pre>\n<h3>Memory Systems</h3>\n<p>Agents need memory to maintain coherence across long interactions:</p>\n<ul>\n<li><strong>Short-term memory</strong>: Current conversation context</li>\n<li><strong>Long-term memory</strong>: Persistent storage of facts, preferences, and past interactions</li>\n<li><strong>Episodic memory</strong>: Recollection of specific past events or tasks</li>\n<li><strong>Semantic memory</strong>: General knowledge and learned concepts</li>\n</ul>\n<h3>Planning Strategies</h3>\n<p>Different approaches to breaking down complex tasks:</p>\n<ul>\n<li><strong>ReAct (Reasoning + Acting)</strong>: Interleave reasoning traces with actions</li>\n<li><strong>Chain of Thought</strong>: Step-by-step reasoning before acting</li>\n<li><strong>Tree of Thoughts</strong>: Explore multiple reasoning paths</li>\n<li><strong>Plan-and-Execute</strong>: Create a full plan first, then execute</li>\n</ul>\n<h2>The Agentic Ecosystem</h2>\n<p>The field is rapidly evolving with frameworks, protocols, and tools designed to make building agents easier and more reliable.</p>\n<h3>Agent Frameworks</h3>\n<p>Several frameworks have emerged to simplify agent development:</p>\n<p><strong>Pydantic AI</strong> - A Python framework that leverages Pydantic's type system for building type-safe, validated agents. It emphasizes developer experience with clean APIs and strong typing.</p>\n<p><strong>AWS Bedrock Agents</strong> - Amazon's managed service for building agents that can securely connect to enterprise data sources and take actions. Provides infrastructure for production-grade agent deployments.</p>\n<p><strong>LangGraph</strong> - A framework for building stateful, multi-actor applications with LLMs, supporting complex agent workflows and coordination.</p>\n<h3>Interoperability Protocols</h3>\n<p>As agents become more sophisticated, standards for communication become crucial:</p>\n<p><strong>Model Context Protocol (MCP)</strong> - An open protocol developed by Anthropic for connecting AI assistants to external data sources and tools. MCP provides a standardized way to expose capabilities to LLMs.</p>\n<p><strong>Agent2Agent (A2A)</strong> - Google's protocol enabling different AI agents to communicate and collaborate, regardless of their underlying frameworks or vendors.</p>\n<h3>Observability and Monitoring</h3>\n<p>Production agents require robust monitoring:</p>\n<p><strong>Langfuse</strong> - An open-source observability platform for LLM applications. It provides tracing, analytics, and evaluation tools specifically designed for understanding agent behavior and debugging issues.</p>\n<h2>Why Agentic Architecture Matters</h2>\n<h3>Current Applications</h3>\n<p>Agents are already transforming various domains:</p>\n<ul>\n<li><strong>Coding Assistants</strong>: Autonomous code generation, debugging, and refactoring</li>\n<li><strong>Research Agents</strong>: Literature review, data analysis, and report generation</li>\n<li><strong>Customer Service</strong>: Complex issue resolution with tool access</li>\n<li><strong>Data Analysis</strong>: End-to-end analysis from data collection to visualization</li>\n<li><strong>Personal Assistants</strong>: Calendar management, email handling, task automation</li>\n</ul>\n<h3>Challenges and Considerations</h3>\n<p>Building reliable agents comes with unique challenges:</p>\n<ul>\n<li><strong>Reliability</strong>: Ensuring consistent behavior across diverse inputs</li>\n<li><strong>Safety</strong>: Preventing harmful actions, especially with tool access</li>\n<li><strong>Cost</strong>: Managing token usage and API costs at scale</li>\n<li><strong>Latency</strong>: Balancing thoroughness with response time</li>\n<li><strong>Evaluation</strong>: Measuring agent performance on open-ended tasks</li>\n</ul>\n<h2>Getting Started</h2>\n<p>If you're new to building agents, here's a recommended learning path:</p>\n<ol>\n<li><strong>Understand LLM fundamentals</strong> - Function calling, prompting techniques</li>\n<li><strong>Start with a framework</strong> - Pick one (Pydantic AI is great for Python devs)</li>\n<li><strong>Build a simple agent</strong> - A single-tool agent for a specific task</li>\n<li><strong>Add complexity gradually</strong> - Multiple tools, memory, planning</li>\n<li><strong>Implement observability</strong> - Add Langfuse or similar from the start</li>\n<li><strong>Explore protocols</strong> - Understand MCP and A2A for interoperability</li>\n</ol>\n<h2>Topics to Explore</h2>\n<p>This introduction sets the stage for deeper dives into specific aspects of agentic architecture:</p>\n<ul>\n<li><a href=\"pydantic-ai.html\">Pydantic AI</a> - Type-safe agent development in Python</li>\n<li><a href=\"bedrock-agents.html\">AWS Bedrock Agents</a> - Enterprise-grade managed agents</li>\n<li><a href=\"mcp.html\">Model Context Protocol</a> - Standardized tool and data integration</li>\n<li><a href=\"a2a.html\">Agent2Agent Protocol</a> - Multi-agent communication</li>\n<li><a href=\"langfuse.html\">Langfuse</a> - Observability for LLM applications</li>\n<li><a href=\"agent-patterns.html\">Agent Design Patterns</a> - Common architectures and best practices</li>\n</ul>\n<h2>Key Takeaways</h2>\n<ul>\n<li>Agentic systems extend LLMs with autonomy, tools, planning, and memory</li>\n<li>The observe-think-act loop is fundamental to agent behavior</li>\n<li>Multiple frameworks exist with different tradeoffs (Pydantic AI, Bedrock, etc.)</li>\n<li>Protocols like MCP and A2A enable interoperability</li>\n<li>Observability (Langfuse) is essential for production systems</li>\n<li>Start simple and add complexity as needed</li>\n</ul>\n<h2>Resources</h2>\n<h3>Documentation</h3>\n<ul>\n<li><a href=\"https://modelcontextprotocol.io/\">Anthropic MCP Documentation</a></li>\n<li><a href=\"https://ai.pydantic.dev/\">Pydantic AI Docs</a></li>\n<li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html\">AWS Bedrock Agents</a></li>\n<li><a href=\"https://langfuse.com/docs\">Langfuse Documentation</a></li>\n</ul>\n<h3>Papers and Research</h3>\n<ul>\n<li>\"ReAct: Synergizing Reasoning and Acting in Language Models\"</li>\n<li>\"Toolformer: Language Models Can Teach Themselves to Use Tools\"</li>\n<li>\"Generative Agents: Interactive Simulacra of Human Behavior\"</li>\n</ul>\n<hr />\n<p><strong>Last Updated:</strong> November 25, 2025<br />\n<strong>Difficulty Level:</strong> Intermediate</p>",
      "excerpt": "# Introduction to Agentic Architecture **This article is generated by Claude Opus 4.5** ## Overview Agentic architecture represents a paradigm shift in how we build AI systems. Instead of simple promp..."
    },
    {
      "id": "dl",
      "name": "Introduction to Deep Learning",
      "category": "deep-learning",
      "tags": [
        "deep learning",
        "neural networks",
        "PyTorch",
        "CNN",
        "RNN",
        "attention",
        "ai-generated"
      ],
      "date": "2025-11-24",
      "description": "Comprehensive introduction to deep learning concepts including neural networks, CNNs, RNNs, attention mechanisms, and backpropagation with PyTorch examples",
      "related": [
        "transformers"
      ],
      "file": "dl.md",
      "html": "<h1>Introduction to Deep Learning</h1>\n<p><strong>This article is generated by Claude Opus 4.5</strong></p>\n<h2>Overview</h2>\n<p>Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to learn complex patterns from data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.</p>\n<h2>What Makes Deep Learning Different?</h2>\n<p>While traditional machine learning requires manual feature engineering, deep learning automatically learns hierarchical representations from raw data. Each layer learns increasingly abstract features.</p>\n<h3>Key Concepts</h3>\n<ul>\n<li><strong>Neural Network</strong>: A computational model inspired by biological neurons</li>\n<li><strong>Layers</strong>: Building blocks that transform input data</li>\n<li><strong>Weights &amp; Biases</strong>: Learnable parameters that the network optimizes</li>\n<li><strong>Activation Functions</strong>: Non-linear functions that enable learning complex patterns</li>\n<li><strong>Loss Function</strong>: Measures how well the model's predictions match the targets</li>\n<li><strong>Backpropagation</strong>: Algorithm for computing gradients to update weights</li>\n</ul>\n<h2>Getting Started with PyTorch</h2>\n<p>PyTorch is a popular deep learning framework known for its flexibility and Pythonic design.</p>\n<h3>Installation</h3>\n<pre><code class=\"language-bash\">pip install torch torchvision\n</code></pre>\n<h3>Basic Tensor Operations</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Create tensors\nx = torch.tensor([1.0, 2.0, 3.0])\ny = torch.tensor([4.0, 5.0, 6.0])\n\n# Basic operations\nz = x + y           # Element-wise addition\ndot = torch.dot(x, y)  # Dot product\n\n# Create a matrix\nmatrix = torch.randn(3, 4)  # Random 3x4 matrix\nprint(f&quot;Matrix shape: {matrix.shape}&quot;)\n\n# GPU support (if available)\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\ntensor_gpu = matrix.to(device)\n</code></pre>\n<h2>Neural Network Fundamentals</h2>\n<h3>The Perceptron (Single Neuron)</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\nclass Perceptron(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n        self.activation = nn.Sigmoid()\n\n    def forward(self, x):\n        return self.activation(self.linear(x))\n\n# Create and use perceptron\nperceptron = Perceptron(input_size=3)\ninput_data = torch.tensor([1.0, 2.0, 3.0])\noutput = perceptron(input_data)\nprint(f&quot;Output: {output.item():.4f}&quot;)\n</code></pre>\n<h3>Multi-Layer Perceptron (MLP)</h3>\n<pre><code class=\"language-python\">class MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, output_size)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\n# Example: Classification with 10 input features, 64 hidden units, 3 classes\nmodel = MLP(input_size=10, hidden_size=64, output_size=3)\nsample_input = torch.randn(1, 10)  # Batch size 1\noutput = model(sample_input)\nprint(f&quot;Output shape: {output.shape}&quot;)  # [1, 3]\n</code></pre>\n<h2>Backpropagation</h2>\n<p>Backpropagation is the algorithm used to compute gradients of the loss with respect to each weight. It applies the chain rule of calculus to efficiently propagate errors backward through the network.</p>\n<h3>How Backpropagation Works</h3>\n<ol>\n<li><strong>Forward Pass</strong>: Compute predictions by passing input through the network</li>\n<li><strong>Compute Loss</strong>: Calculate the difference between predictions and targets</li>\n<li><strong>Backward Pass</strong>: Compute gradients using the chain rule</li>\n<li><strong>Update Weights</strong>: Adjust weights in the direction that reduces loss</li>\n</ol>\n<h3>Manual Backpropagation Example</h3>\n<pre><code class=\"language-python\">import torch\n\n# Simple network: y = w2 * relu(w1 * x + b1) + b2\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\ntarget = torch.tensor([1.0])\n\n# Initialize weights with gradient tracking\nw1 = torch.randn(3, 4, requires_grad=True)\nb1 = torch.zeros(4, requires_grad=True)\nw2 = torch.randn(4, 1, requires_grad=True)\nb2 = torch.zeros(1, requires_grad=True)\n\n# Forward pass\nhidden = torch.relu(x @ w1 + b1)\noutput = hidden @ w2 + b2\n\n# Compute loss (MSE)\nloss = ((output - target) ** 2).mean()\nprint(f&quot;Loss: {loss.item():.4f}&quot;)\n\n# Backward pass - computes all gradients\nloss.backward()\n\n# Gradients are now available\nprint(f&quot;w1 gradient shape: {w1.grad.shape}&quot;)\nprint(f&quot;w2 gradient shape: {w2.grad.shape}&quot;)\n</code></pre>\n<h3>Training Loop with PyTorch</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Create model, loss function, and optimizer\nmodel = MLP(input_size=10, hidden_size=64, output_size=3)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndef train_step(model, X_batch, y_batch):\n    model.train()\n\n    # Forward pass\n    outputs = model(X_batch)\n    loss = criterion(outputs, y_batch)\n\n    # Backward pass\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients\n    optimizer.step()       # Update weights\n\n    return loss.item()\n\n# Example training\nX_train = torch.randn(100, 10)  # 100 samples\ny_train = torch.randint(0, 3, (100,))  # 3 classes\n\nfor epoch in range(10):\n    loss = train_step(model, X_train, y_train)\n    print(f&quot;Epoch {epoch+1}, Loss: {loss:.4f}&quot;)\n</code></pre>\n<h2>Convolutional Neural Networks (CNNs)</h2>\n<p>CNNs are designed for processing grid-like data such as images. They use convolutional layers to automatically learn spatial hierarchies of features.</p>\n<h3>Key Components</h3>\n<ul>\n<li><strong>Convolutional Layer</strong>: Applies filters to detect local patterns</li>\n<li><strong>Pooling Layer</strong>: Reduces spatial dimensions while retaining important features</li>\n<li><strong>Stride</strong>: Step size when sliding the filter</li>\n<li><strong>Padding</strong>: Adding zeros around input to control output size</li>\n</ul>\n<h3>CNN Architecture in PyTorch</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(\n            in_channels=1,    # Grayscale image\n            out_channels=32,  # 32 filters\n            kernel_size=3,    # 3x3 filter\n            padding=1         # Same padding\n        )\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n\n        # Pooling layer\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Batch normalization\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 3 * 3, 256)  # After 3 pooling ops: 28-&gt;14-&gt;7-&gt;3\n        self.fc2 = nn.Linear(256, num_classes)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # Conv block 1: [B, 1, 28, 28] -&gt; [B, 32, 14, 14]\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n\n        # Conv block 2: [B, 32, 14, 14] -&gt; [B, 64, 7, 7]\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n\n        # Conv block 3: [B, 64, 7, 7] -&gt; [B, 128, 3, 3]\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n\n        # Flatten: [B, 128, 3, 3] -&gt; [B, 128*3*3]\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.fc2(x)\n\n        return x\n\n# Create model and test with random image\ncnn = CNN(num_classes=10)\nsample_image = torch.randn(1, 1, 28, 28)  # Batch=1, Channels=1, H=28, W=28\noutput = cnn(sample_image)\nprint(f&quot;CNN output shape: {output.shape}&quot;)  # [1, 10]\n</code></pre>\n<h3>Understanding Convolutions</h3>\n<pre><code class=\"language-python\"># Visualize what a convolution does\nimport torch\nimport torch.nn as nn\n\n# Create a simple edge detection filter\nedge_filter = torch.tensor([\n    [-1, -1, -1],\n    [-1,  8, -1],\n    [-1, -1, -1]\n], dtype=torch.float32).view(1, 1, 3, 3)\n\n# Apply to a sample image\nconv = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\nconv.weight.data = edge_filter\n\nsample = torch.randn(1, 1, 8, 8)\nfiltered = conv(sample)\nprint(f&quot;Input shape: {sample.shape}, Output shape: {filtered.shape}&quot;)\n</code></pre>\n<h2>Recurrent Neural Networks (RNNs)</h2>\n<p>RNNs are designed for sequential data where the order matters (text, time series, audio). They maintain a hidden state that captures information from previous time steps.</p>\n<h3>Basic RNN</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\n\nclass SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n\n        # RNN layer\n        self.rnn = nn.RNN(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True  # Input shape: [batch, seq_len, features]\n        )\n\n        # Output layer\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden=None):\n        # x shape: [batch, seq_len, input_size]\n\n        # Initialize hidden state if not provided\n        if hidden is None:\n            hidden = torch.zeros(1, x.size(0), self.hidden_size)\n\n        # RNN forward pass\n        output, hidden = self.rnn(x, hidden)\n        # output shape: [batch, seq_len, hidden_size]\n        # hidden shape: [num_layers, batch, hidden_size]\n\n        # Take the last output for classification\n        last_output = output[:, -1, :]  # [batch, hidden_size]\n        prediction = self.fc(last_output)\n\n        return prediction, hidden\n\n# Example: Sequence classification\nrnn = SimpleRNN(input_size=10, hidden_size=64, output_size=5)\nsequence = torch.randn(32, 20, 10)  # Batch=32, SeqLen=20, Features=10\noutput, _ = rnn(sequence)\nprint(f&quot;RNN output shape: {output.shape}&quot;)  # [32, 5]\n</code></pre>\n<h3>LSTM (Long Short-Term Memory)</h3>\n<p>LSTMs solve the vanishing gradient problem in vanilla RNNs using gates that control information flow.</p>\n<pre><code class=\"language-python\">class LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes):\n        super().__init__()\n\n        # Embedding layer for text\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n        # LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_size,\n            num_layers=2,\n            batch_first=True,\n            dropout=0.3,\n            bidirectional=True  # Process sequence both directions\n        )\n\n        # Output layer (bidirectional doubles hidden size)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, x):\n        # x shape: [batch, seq_len] - token indices\n\n        # Embed tokens\n        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]\n\n        # LSTM forward pass\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n        # lstm_out: [batch, seq_len, hidden_size*2]\n        # hidden: [num_layers*2, batch, hidden_size]\n\n        # Concatenate final hidden states from both directions\n        hidden_cat = torch.cat([hidden[-2], hidden[-1]], dim=1)\n\n        # Classification\n        output = self.fc(hidden_cat)\n        return output\n\n# Example: Text classification\nlstm_model = LSTMClassifier(vocab_size=10000, embed_dim=128, hidden_size=256, num_classes=3)\ntext_input = torch.randint(0, 10000, (16, 50))  # Batch=16, SeqLen=50\noutput = lstm_model(text_input)\nprint(f&quot;LSTM output shape: {output.shape}&quot;)  # [16, 3]\n</code></pre>\n<h3>GRU (Gated Recurrent Unit)</h3>\n<p>GRUs are a simpler alternative to LSTMs with fewer parameters.</p>\n<pre><code class=\"language-python\">class GRUModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n        super().__init__()\n\n        self.gru = nn.GRU(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=0.2\n        )\n\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        output, hidden = self.gru(x)\n        # Use last time step output\n        return self.fc(output[:, -1, :])\n\n# Time series prediction\ngru = GRUModel(input_size=1, hidden_size=64, output_size=1)\ntime_series = torch.randn(32, 100, 1)  # 100 time steps\nprediction = gru(time_series)\nprint(f&quot;GRU prediction shape: {prediction.shape}&quot;)  # [32, 1]\n</code></pre>\n<h2>Attention Mechanisms</h2>\n<p>Attention allows models to focus on relevant parts of the input when making predictions. It's the foundation of modern architectures like Transformers.</p>\n<h3>Basic Attention</h3>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Attention(nn.Module):\n    &quot;&quot;&quot;Simple additive attention mechanism.&quot;&quot;&quot;\n\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attention = nn.Linear(hidden_size, 1)\n\n    def forward(self, encoder_outputs):\n        # encoder_outputs: [batch, seq_len, hidden_size]\n\n        # Compute attention scores\n        scores = self.attention(encoder_outputs)  # [batch, seq_len, 1]\n        scores = scores.squeeze(-1)  # [batch, seq_len]\n\n        # Softmax to get attention weights\n        weights = F.softmax(scores, dim=1)  # [batch, seq_len]\n\n        # Weighted sum of encoder outputs\n        context = torch.bmm(\n            weights.unsqueeze(1),  # [batch, 1, seq_len]\n            encoder_outputs         # [batch, seq_len, hidden_size]\n        ).squeeze(1)  # [batch, hidden_size]\n\n        return context, weights\n</code></pre>\n<h3>Scaled Dot-Product Attention</h3>\n<p>This is the attention mechanism used in Transformers.</p>\n<pre><code class=\"language-python\">class ScaledDotProductAttention(nn.Module):\n    def __init__(self, d_k):\n        super().__init__()\n        self.scale = d_k ** 0.5\n\n    def forward(self, query, key, value, mask=None):\n        &quot;&quot;&quot;\n        Args:\n            query: [batch, seq_len, d_k]\n            key: [batch, seq_len, d_k]\n            value: [batch, seq_len, d_v]\n            mask: Optional mask for padding or causal attention\n        &quot;&quot;&quot;\n        # Compute attention scores\n        scores = torch.bmm(query, key.transpose(1, 2)) / self.scale\n        # scores: [batch, seq_len, seq_len]\n\n        # Apply mask if provided\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n\n        # Softmax to get attention weights\n        attention_weights = F.softmax(scores, dim=-1)\n\n        # Weighted sum of values\n        output = torch.bmm(attention_weights, value)\n\n        return output, attention_weights\n\n# Example usage\nd_k = 64\nattention = ScaledDotProductAttention(d_k)\n\nbatch_size, seq_len = 8, 20\nQ = torch.randn(batch_size, seq_len, d_k)\nK = torch.randn(batch_size, seq_len, d_k)\nV = torch.randn(batch_size, seq_len, d_k)\n\noutput, weights = attention(Q, K, V)\nprint(f&quot;Attention output shape: {output.shape}&quot;)  # [8, 20, 64]\nprint(f&quot;Attention weights shape: {weights.shape}&quot;)  # [8, 20, 20]\n</code></pre>\n<h3>Multi-Head Attention</h3>\n<p>Multi-head attention allows the model to attend to different representation subspaces.</p>\n<pre><code class=\"language-python\">class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        # Linear projections for Q, K, V\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n\n        # Output projection\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.attention = ScaledDotProductAttention(self.d_k)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # Linear projections and reshape for multi-head\n        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        # Q, K, V shape: [batch, num_heads, seq_len, d_k]\n\n        # Reshape for batch matrix multiplication\n        Q = Q.reshape(batch_size * self.num_heads, -1, self.d_k)\n        K = K.reshape(batch_size * self.num_heads, -1, self.d_k)\n        V = V.reshape(batch_size * self.num_heads, -1, self.d_k)\n\n        # Apply attention\n        attended, weights = self.attention(Q, K, V, mask)\n\n        # Reshape back\n        attended = attended.view(batch_size, self.num_heads, -1, self.d_k)\n        attended = attended.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n\n        # Final linear projection\n        output = self.W_o(attended)\n\n        return output\n\n# Example\nmha = MultiHeadAttention(d_model=512, num_heads=8)\nx = torch.randn(4, 30, 512)  # Batch=4, SeqLen=30, Dim=512\noutput = mha(x, x, x)  # Self-attention\nprint(f&quot;Multi-head attention output: {output.shape}&quot;)  # [4, 30, 512]\n</code></pre>\n<h3>Self-Attention with RNN</h3>\n<p>Combining attention with RNNs for sequence classification.</p>\n<pre><code class=\"language-python\">class AttentionRNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n        self.lstm = nn.LSTM(\n            embed_dim, hidden_size,\n            batch_first=True, bidirectional=True\n        )\n\n        self.attention = Attention(hidden_size * 2)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, x):\n        # Embed\n        embedded = self.embedding(x)\n\n        # LSTM encoding\n        lstm_out, _ = self.lstm(embedded)\n\n        # Apply attention to get context vector\n        context, attention_weights = self.attention(lstm_out)\n\n        # Classify\n        output = self.fc(context)\n\n        return output, attention_weights\n\n# Example\nmodel = AttentionRNN(vocab_size=5000, embed_dim=128, hidden_size=128, num_classes=2)\ntext = torch.randint(0, 5000, (8, 50))\noutput, attn_weights = model(text)\nprint(f&quot;Output: {output.shape}, Attention: {attn_weights.shape}&quot;)\n</code></pre>\n<h2>Complete Training Example</h2>\n<p>Here's a full example training a CNN on image classification:</p>\n<pre><code class=\"language-python\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Model\nclass ImageClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.classifier = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n\n# Training function\ndef train(model, train_loader, epochs=10, device='cpu'):\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n\n        for batch_x, batch_y in train_loader:\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(batch_y).sum().item()\n            total += batch_y.size(0)\n\n        scheduler.step()\n\n        accuracy = 100. * correct / total\n        print(f&quot;Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f} - Accuracy: {accuracy:.2f}%&quot;)\n\n# Create sample data and train\nX = torch.randn(1000, 3, 32, 32)  # 1000 RGB images 32x32\ny = torch.randint(0, 10, (1000,))  # 10 classes\n\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nmodel = ImageClassifier()\ntrain(model, loader, epochs=5)\n</code></pre>\n<h2>Activation Functions</h2>\n<pre><code class=\"language-python\">import torch\nimport torch.nn.functional as F\n\nx = torch.linspace(-5, 5, 100)\n\n# Common activation functions\nrelu = F.relu(x)           # max(0, x)\nsigmoid = torch.sigmoid(x)  # 1 / (1 + e^-x)\ntanh = torch.tanh(x)        # (e^x - e^-x) / (e^x + e^-x)\nleaky_relu = F.leaky_relu(x, 0.01)  # x if x &gt; 0 else 0.01*x\ngelu = F.gelu(x)            # Used in Transformers\n\n# In a model\nmodel = nn.Sequential(\n    nn.Linear(10, 64),\n    nn.GELU(),  # or nn.ReLU(), nn.LeakyReLU(), etc.\n    nn.Linear(64, 10)\n)\n</code></pre>\n<h2>Loss Functions</h2>\n<pre><code class=\"language-python\"># Classification\nce_loss = nn.CrossEntropyLoss()      # Multi-class\nbce_loss = nn.BCEWithLogitsLoss()    # Binary\n\n# Regression\nmse_loss = nn.MSELoss()              # Mean Squared Error\nmae_loss = nn.L1Loss()               # Mean Absolute Error\nhuber_loss = nn.SmoothL1Loss()       # Robust to outliers\n\n# Sequence models\nctc_loss = nn.CTCLoss()              # Speech recognition\n\n# Example usage\npredictions = torch.randn(32, 10)    # 32 samples, 10 classes\ntargets = torch.randint(0, 10, (32,))\nloss = ce_loss(predictions, targets)\n</code></pre>\n<h2>Key Takeaways</h2>\n<ul>\n<li>Deep learning uses multiple layers to learn hierarchical representations</li>\n<li>Backpropagation efficiently computes gradients using the chain rule</li>\n<li>CNNs excel at spatial data (images) using convolutional filters</li>\n<li>RNNs/LSTMs/GRUs process sequential data with memory</li>\n<li>Attention mechanisms allow models to focus on relevant input parts</li>\n<li>PyTorch provides a flexible framework for building and training models</li>\n<li>Always use GPU acceleration when available for faster training</li>\n</ul>\n<h2>Resources for Learning</h2>\n<h3>Online Courses</h3>\n<ul>\n<li><a href=\"https://www.coursera.org/specializations/deep-learning\">Deep Learning Specialization (Coursera)</a></li>\n<li><a href=\"https://course.fast.ai/\">Fast.ai Practical Deep Learning</a></li>\n<li><a href=\"http://cs231n.stanford.edu/\">Stanford CS231n: CNNs for Visual Recognition</a></li>\n<li><a href=\"http://web.stanford.edu/class/cs224n/\">Stanford CS224n: NLP with Deep Learning</a></li>\n</ul>\n<h3>Books</h3>\n<ul>\n<li>\"Deep Learning\" by Goodfellow, Bengio, and Courville</li>\n<li>\"Dive into Deep Learning\" (d2l.ai) - Free online book</li>\n<li>\"Neural Networks and Deep Learning\" by Michael Nielsen</li>\n</ul>\n<h3>Documentation</h3>\n<ul>\n<li><a href=\"https://pytorch.org/docs/\">PyTorch Documentation</a></li>\n<li><a href=\"https://pytorch.org/tutorials/\">PyTorch Tutorials</a></li>\n</ul>\n<h2>Related Topics</h2>\n<ul>\n<li><a href=\"ml.html\">Machine Learning Fundamentals</a> - ML basics before diving into DL</li>\n<li><a href=\"python.html\">Python Basics</a> - Programming foundation</li>\n<li><a href=\"transformers.html\">Transformers</a> - Modern attention-based architectures</li>\n</ul>\n<h2>Tools and Libraries</h2>\n<ul>\n<li><strong>PyTorch</strong>: Flexible deep learning framework</li>\n<li><strong>TensorFlow/Keras</strong>: Alternative framework by Google</li>\n<li><strong>Hugging Face</strong>: Pre-trained models and datasets</li>\n<li><strong>Weights &amp; Biases</strong>: Experiment tracking</li>\n<li><strong>TensorBoard</strong>: Visualization toolkit</li>\n</ul>\n<hr />\n<p><strong>Last Updated:</strong> November 24, 2025<br />\n<strong>Difficulty Level:</strong> Intermediate</p>",
      "excerpt": "# Introduction to Deep Learning **This article is generated by Claude Opus 4.5** ## Overview Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers (he..."
    },
    {
      "id": "ml",
      "name": "Machine Learning Fundamentals",
      "category": "machine-learning",
      "tags": [
        "machine learning",
        "AI",
        "algorithms",
        "supervised learning",
        "ai-generated"
      ],
      "date": "2025-11-08",
      "description": "Introduction to machine learning concepts, algorithms, and applications",
      "related": [
        "python",
        "dl"
      ],
      "file": "ml.md",
      "html": "<h1>Machine Learning Fundamentals</h1>\n<p><strong>This article is generated by Claude Opus 4.5</strong></p>\n<h2>Overview</h2>\n<p>Machine Learning (ML) is a subset of artificial intelligence that focuses on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed, these systems improve their performance through experience.</p>\n<h2>What is Machine Learning?</h2>\n<p>Machine Learning enables computers to learn patterns from data without being explicitly programmed for every scenario. It's the foundation of many modern AI applications, from recommendation systems to autonomous vehicles.</p>\n<h3>Key Concepts</h3>\n<ul>\n<li><strong>Training Data</strong>: Historical data used to teach the model</li>\n<li><strong>Features</strong>: Input variables used to make predictions</li>\n<li><strong>Labels</strong>: The output we want to predict (in supervised learning)</li>\n<li><strong>Model</strong>: The mathematical representation of patterns in data</li>\n</ul>\n<h2>Types of Machine Learning</h2>\n<h3>1. Supervised Learning</h3>\n<p>The algorithm learns from labeled training data, making predictions based on that data.</p>\n<p><strong>Examples:</strong>\n- Classification (spam detection, image recognition)\n- Regression (price prediction, weather forecasting)</p>\n<pre><code class=\"language-python\"># Simple supervised learning example with sklearn\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Training data\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([2, 4, 6, 8, 10])\n\n# Create and train model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Make prediction\nprediction = model.predict([[6]])\nprint(f&quot;Prediction for 6: {prediction[0]}&quot;)  # Output: ~12\n</code></pre>\n<h3>2. Unsupervised Learning</h3>\n<p>The algorithm finds patterns in unlabeled data.</p>\n<p><strong>Examples:</strong>\n- Clustering (customer segmentation)\n- Dimensionality reduction (data compression)\n- Anomaly detection</p>\n<h3>3. Reinforcement Learning</h3>\n<p>The algorithm learns through trial and error, receiving rewards or penalties.</p>\n<p><strong>Examples:</strong>\n- Game playing (AlphaGo, Chess engines)\n- Robotics\n- Resource optimization</p>\n<h2>Common Algorithms</h2>\n<h3>Linear Regression</h3>\n<p>Predicts continuous values based on linear relationships.</p>\n<h3>Logistic Regression</h3>\n<p>Classification algorithm for binary outcomes.</p>\n<h3>Decision Trees</h3>\n<p>Tree-like model for classification and regression.</p>\n<h3>Neural Networks</h3>\n<p>Inspired by biological neurons, forms the basis of deep learning.</p>\n<h3>Support Vector Machines (SVM)</h3>\n<p>Finds optimal boundaries between classes.</p>\n<h3>K-Means Clustering</h3>\n<p>Groups similar data points together.</p>\n<h2>The ML Workflow</h2>\n<ol>\n<li><strong>Problem Definition</strong>: What are we trying to predict or understand?</li>\n<li><strong>Data Collection</strong>: Gather relevant data</li>\n<li><strong>Data Preprocessing</strong>: Clean and prepare data</li>\n<li><strong>Feature Engineering</strong>: Create meaningful features</li>\n<li><strong>Model Selection</strong>: Choose appropriate algorithm</li>\n<li><strong>Training</strong>: Teach the model with training data</li>\n<li><strong>Evaluation</strong>: Test model performance</li>\n<li><strong>Deployment</strong>: Use the model in production</li>\n<li><strong>Monitoring</strong>: Track performance over time</li>\n</ol>\n<h2>Practical Example: Predicting House Prices</h2>\n<pre><code class=\"language-python\">import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Load data\ndata = pd.DataFrame({\n    'size': [1500, 1600, 1700, 1800, 1900],\n    'bedrooms': [3, 3, 4, 4, 5],\n    'age': [10, 15, 20, 5, 8],\n    'price': [300000, 320000, 340000, 380000, 400000]\n})\n\n# Prepare features and target\nX = data[['size', 'bedrooms', 'age']]\ny = data['price']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train model\nmodel = RandomForestRegressor(n_estimators=100)\nmodel.fit(X_train, y_train)\n\n# Evaluate\npredictions = model.predict(X_test)\nmse = mean_squared_error(y_test, predictions)\nprint(f&quot;Mean Squared Error: {mse}&quot;)\n</code></pre>\n<h2>Resources for Learning</h2>\n<h3>Online Courses</h3>\n<ul>\n<li><a href=\"https://www.coursera.org/learn/machine-learning\">Coursera: Machine Learning by Andrew Ng</a></li>\n<li><a href=\"https://www.fast.ai/\">Fast.ai: Practical Deep Learning</a></li>\n</ul>\n<h3>Books</h3>\n<ul>\n<li>\"Hands-On Machine Learning\" by Aur\u00e9lien G\u00e9ron</li>\n<li>\"Pattern Recognition and Machine Learning\" by Christopher Bishop</li>\n</ul>\n<h3>Practice Platforms</h3>\n<ul>\n<li><a href=\"https://www.kaggle.com\">Kaggle</a> - Competitions and datasets</li>\n<li><a href=\"https://colab.research.google.com\">Google Colab</a> - Free GPU for experiments</li>\n</ul>\n<h2>Key Takeaways</h2>\n<ul>\n<li>Machine Learning enables computers to learn from data</li>\n<li>Three main types: Supervised, Unsupervised, and Reinforcement Learning</li>\n<li>The ML workflow is iterative and requires continuous improvement</li>\n<li>Start with simple algorithms before moving to complex ones</li>\n<li>Practice is essential - work on real projects and datasets</li>\n</ul>\n<h2>Related Topics</h2>\n<ul>\n<li><a href=\"dl.html\">Deep Learning</a> - Neural networks and advanced architectures</li>\n<li><a href=\"python.html\">Python Basics</a> - Programming fundamentals for ML</li>\n<li><a href=\"statistics.html\">Statistics</a> - Mathematical foundations</li>\n</ul>\n<h2>Tools and Libraries</h2>\n<ul>\n<li><strong>Scikit-learn</strong>: General-purpose ML library</li>\n<li><strong>TensorFlow</strong>: Deep learning framework</li>\n<li><strong>PyTorch</strong>: Deep learning framework</li>\n<li><strong>Pandas</strong>: Data manipulation</li>\n<li><strong>NumPy</strong>: Numerical computing</li>\n</ul>\n<hr />\n<p><strong>Last Updated:</strong> November 8, 2025<br />\n<strong>Difficulty Level:</strong> Beginner to Intermediate</p>",
      "excerpt": "# Machine Learning Fundamentals **This article is generated by Claude Opus 4.5** ## Overview Machine Learning (ML) is a subset of artificial intelligence that focuses on building systems that can lear..."
    },
    {
      "id": "python",
      "name": "Python Programming Basics",
      "category": "machine-learning",
      "tags": [
        "python",
        "programming",
        "basics",
        "tutorial",
        "ai-generated"
      ],
      "date": "2025-11-08",
      "description": "Essential Python programming concepts for beginners",
      "related": [
        "ml"
      ],
      "file": "python.md",
      "html": "<h1>Python Programming Basics</h1>\n<p><strong>This article is generated by Claude Opus 4.5</strong></p>\n<h2>Overview</h2>\n<p>Python is a high-level, interpreted programming language known for its simplicity and readability. It's one of the most popular languages for data science, machine learning, web development, and automation.</p>\n<h2>Why Python?</h2>\n<ul>\n<li><strong>Easy to Learn</strong>: Clean, readable syntax</li>\n<li><strong>Versatile</strong>: Web dev, data science, automation, AI</li>\n<li><strong>Large Community</strong>: Extensive libraries and support</li>\n<li><strong>Cross-platform</strong>: Works on Windows, Mac, Linux</li>\n<li><strong>Free and Open Source</strong>: No licensing costs</li>\n</ul>\n<h2>Getting Started</h2>\n<h3>Installation</h3>\n<p>Visit <a href=\"https://www.python.org/downloads/\">python.org</a> to download Python.</p>\n<pre><code class=\"language-bash\"># Check if Python is installed\npython --version\n\n# Or\npython3 --version\n</code></pre>\n<h3>Your First Program</h3>\n<pre><code class=\"language-python\"># hello.py\nprint(&quot;Hello, World!&quot;)\n</code></pre>\n<p>Run it:</p>\n<pre><code class=\"language-bash\">python hello.py\n</code></pre>\n<h2>Basic Syntax</h2>\n<h3>Variables</h3>\n<pre><code class=\"language-python\"># Variables (no declaration needed)\nname = &quot;Alice&quot;\nage = 25\nheight = 5.6\nis_student = True\n\n# Multiple assignment\nx, y, z = 1, 2, 3\n</code></pre>\n<h3>Data Types</h3>\n<pre><code class=\"language-python\"># Integers\ncount = 10\n\n# Floats\nprice = 19.99\n\n# Strings\nmessage = &quot;Hello, Python!&quot;\n\n# Booleans\nis_active = True\n\n# Lists\nfruits = [&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;]\n\n# Tuples (immutable)\ncoordinates = (10, 20)\n\n# Dictionaries\nperson = {\n    &quot;name&quot;: &quot;Alice&quot;,\n    &quot;age&quot;: 25,\n    &quot;city&quot;: &quot;New York&quot;\n}\n\n# Sets (unique elements)\nunique_numbers = {1, 2, 3, 4}\n</code></pre>\n<h2>Control Flow</h2>\n<h3>Conditionals</h3>\n<pre><code class=\"language-python\"># if-elif-else\nage = 18\n\nif age &lt; 18:\n    print(&quot;Minor&quot;)\nelif age == 18:\n    print(&quot;Just became an adult&quot;)\nelse:\n    print(&quot;Adult&quot;)\n</code></pre>\n<h3>Loops</h3>\n<pre><code class=\"language-python\"># for loop\nfruits = [&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;]\nfor fruit in fruits:\n    print(fruit)\n\n# range()\nfor i in range(5):  # 0 to 4\n    print(i)\n\n# while loop\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n</code></pre>\n<h2>Functions</h2>\n<pre><code class=\"language-python\"># Basic function\ndef greet(name):\n    return f&quot;Hello, {name}!&quot;\n\nmessage = greet(&quot;Alice&quot;)\nprint(message)  # Hello, Alice!\n\n# Default parameters\ndef power(base, exponent=2):\n    return base ** exponent\n\nprint(power(3))      # 9 (3^2)\nprint(power(3, 3))   # 27 (3^3)\n\n# Multiple return values\ndef get_stats(numbers):\n    return min(numbers), max(numbers), sum(numbers)\n\nminimum, maximum, total = get_stats([1, 2, 3, 4, 5])\n</code></pre>\n<h2>Lists and List Comprehensions</h2>\n<pre><code class=\"language-python\"># List operations\nnumbers = [1, 2, 3, 4, 5]\n\nnumbers.append(6)        # Add to end\nnumbers.insert(0, 0)     # Insert at position\nnumbers.remove(3)        # Remove value\nnumbers.pop()            # Remove last item\n\n# Slicing\nfirst_three = numbers[:3]\nlast_two = numbers[-2:]\n\n# List comprehension\nsquares = [x**2 for x in range(10)]\n# [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\nevens = [x for x in range(20) if x % 2 == 0]\n# [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n</code></pre>\n<h2>Dictionaries</h2>\n<pre><code class=\"language-python\"># Creating and accessing\nstudent = {\n    &quot;name&quot;: &quot;Alice&quot;,\n    &quot;age&quot;: 20,\n    &quot;grades&quot;: [85, 90, 92]\n}\n\nprint(student[&quot;name&quot;])           # Alice\nprint(student.get(&quot;age&quot;))        # 20\nprint(student.get(&quot;email&quot;, &quot;N/A&quot;))  # N/A (default)\n\n# Adding/updating\nstudent[&quot;email&quot;] = &quot;alice@example.com&quot;\nstudent[&quot;age&quot;] = 21\n\n# Iterating\nfor key, value in student.items():\n    print(f&quot;{key}: {value}&quot;)\n</code></pre>\n<h2>Classes and Objects</h2>\n<pre><code class=\"language-python\"># Define a class\nclass Dog:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def bark(self):\n        return f&quot;{self.name} says Woof!&quot;\n\n    def get_age_in_dog_years(self):\n        return self.age * 7\n\n# Create objects\nmy_dog = Dog(&quot;Buddy&quot;, 3)\nprint(my_dog.bark())                    # Buddy says Woof!\nprint(my_dog.get_age_in_dog_years())   # 21\n</code></pre>\n<h2>File Handling</h2>\n<pre><code class=\"language-python\"># Writing to a file\nwith open(&quot;output.txt&quot;, &quot;w&quot;) as file:\n    file.write(&quot;Hello, World!\\n&quot;)\n    file.write(&quot;Python is awesome!&quot;)\n\n# Reading from a file\nwith open(&quot;output.txt&quot;, &quot;r&quot;) as file:\n    content = file.read()\n    print(content)\n\n# Reading line by line\nwith open(&quot;output.txt&quot;, &quot;r&quot;) as file:\n    for line in file:\n        print(line.strip())\n</code></pre>\n<h2>Error Handling</h2>\n<pre><code class=\"language-python\"># try-except\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(&quot;Cannot divide by zero!&quot;)\nexcept Exception as e:\n    print(f&quot;An error occurred: {e}&quot;)\nfinally:\n    print(&quot;This always executes&quot;)\n\n# Raising exceptions\ndef divide(a, b):\n    if b == 0:\n        raise ValueError(&quot;Divisor cannot be zero&quot;)\n    return a / b\n</code></pre>\n<h2>Modules and Imports</h2>\n<pre><code class=\"language-python\"># Importing standard library\nimport math\nprint(math.pi)           # 3.141592653589793\nprint(math.sqrt(16))     # 4.0\n\n# Import specific functions\nfrom math import pi, sqrt\nprint(pi)\n\n# Import with alias\nimport numpy as np\nimport pandas as pd\n\n# Your own modules\n# In mymodule.py:\ndef my_function():\n    return &quot;Hello from module&quot;\n\n# In main.py:\nimport mymodule\nprint(mymodule.my_function())\n</code></pre>\n<h2>Useful Built-in Functions</h2>\n<pre><code class=\"language-python\"># len() - length\nprint(len([1, 2, 3]))        # 3\nprint(len(&quot;Hello&quot;))          # 5\n\n# type() - check type\nprint(type(42))              # &lt;class 'int'&gt;\n\n# range() - sequence of numbers\nlist(range(5))               # [0, 1, 2, 3, 4]\n\n# enumerate() - index and value\nfor i, fruit in enumerate([&quot;apple&quot;, &quot;banana&quot;]):\n    print(f&quot;{i}: {fruit}&quot;)\n\n# zip() - combine iterables\nnames = [&quot;Alice&quot;, &quot;Bob&quot;]\nages = [25, 30]\nfor name, age in zip(names, ages):\n    print(f&quot;{name} is {age}&quot;)\n\n# map() - apply function\nnumbers = [1, 2, 3, 4]\nsquared = list(map(lambda x: x**2, numbers))\n# [1, 4, 9, 16]\n\n# filter() - filter elements\nevens = list(filter(lambda x: x % 2 == 0, numbers))\n# [2, 4]\n</code></pre>\n<h2>Common Mistakes to Avoid</h2>\n<ol>\n<li><strong>Indentation Errors</strong>: Python uses indentation (not brackets)</li>\n<li><strong>Mutable Default Arguments</strong>: Don't use mutable defaults in functions</li>\n<li><strong>Not Closing Files</strong>: Use <code>with</code> statement</li>\n<li><strong>Index Out of Range</strong>: Check list length before accessing</li>\n<li><strong>Integer Division</strong>: Use <code>//</code> for floor division, <code>/</code> for float</li>\n</ol>\n<h2>Practice Projects</h2>\n<h3>Beginner</h3>\n<ul>\n<li>Calculator</li>\n<li>To-do list</li>\n<li>Number guessing game</li>\n<li>Password generator</li>\n</ul>\n<h3>Intermediate</h3>\n<ul>\n<li>Web scraper</li>\n<li>Data analyzer</li>\n<li>Simple game (snake, tic-tac-toe)</li>\n<li>API consumer</li>\n</ul>\n<h2>Essential Libraries</h2>\n<pre><code class=\"language-python\"># Data Science\nimport numpy as np         # Numerical computing\nimport pandas as pd        # Data manipulation\nimport matplotlib.pyplot as plt  # Visualization\n\n# Web Development\nfrom flask import Flask    # Web framework\nimport requests           # HTTP requests\n\n# Automation\nimport os                 # Operating system\nimport sys                # System-specific\nfrom pathlib import Path  # File paths\n</code></pre>\n<h2>Resources for Learning</h2>\n<h3>Online Platforms</h3>\n<ul>\n<li><a href=\"https://docs.python.org/3/tutorial/\">Python.org Tutorial</a></li>\n<li><a href=\"https://realpython.com/\">Real Python</a></li>\n<li><a href=\"https://www.py4e.com/\">Python for Everybody</a></li>\n</ul>\n<h3>Practice</h3>\n<ul>\n<li><a href=\"https://leetcode.com/\">LeetCode</a> - Coding challenges</li>\n<li><a href=\"https://www.hackerrank.com/\">HackerRank</a> - Python track</li>\n<li><a href=\"https://projecteuler.net/\">Project Euler</a> - Math problems</li>\n</ul>\n<h3>Books</h3>\n<ul>\n<li>\"Python Crash Course\" by Eric Matthes</li>\n<li>\"Automate the Boring Stuff with Python\" by Al Sweigart</li>\n<li>\"Fluent Python\" by Luciano Ramalho (Advanced)</li>\n</ul>\n<h2>Google Colab Example</h2>\n<p>Try this code in <a href=\"https://colab.research.google.com\">Google Colab</a>:</p>\n<pre><code class=\"language-python\"># Data analysis example\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create sample data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, 35, 28],\n    'Score': [85, 90, 78, 92]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n# Plot\nplt.bar(df['Name'], df['Score'])\nplt.title('Student Scores')\nplt.xlabel('Name')\nplt.ylabel('Score')\nplt.show()\n</code></pre>\n<h2>Key Takeaways</h2>\n<ul>\n<li>Python is beginner-friendly with clean syntax</li>\n<li>Strong typing but dynamically typed</li>\n<li>Indentation matters (use 4 spaces)</li>\n<li>Rich standard library and ecosystem</li>\n<li>Great for automation, data science, and web development</li>\n<li>Practice is essential - code daily!</li>\n</ul>\n<h2>Related Topics</h2>\n<ul>\n<li><a href=\"ml.html\">Machine Learning</a> - Use Python for ML</li>\n<li><a href=\"numpy.html\">NumPy</a> - Numerical computing library</li>\n<li><a href=\"data-structures.html\">Data Structures</a> - Core CS concepts</li>\n</ul>\n<h2>Next Steps</h2>\n<ol>\n<li>Install Python on your machine</li>\n<li>Complete a beginner tutorial</li>\n<li>Build small projects</li>\n<li>Learn a library (NumPy, Pandas, or Flask)</li>\n<li>Contribute to open source</li>\n</ol>\n<hr />\n<p><strong>Last Updated:</strong> November 8, 2025<br />\n<strong>Difficulty Level:</strong> Beginner</p>",
      "excerpt": "# Python Programming Basics **This article is generated by Claude Opus 4.5** ## Overview Python is a high-level, interpreted programming language known for its simplicity and readability. It's one of ..."
    }
  ],
  "links": [
    {
      "source": "optimization",
      "target": "dl"
    },
    {
      "source": "imageprocess",
      "target": "dl"
    },
    {
      "source": "styletransfer",
      "target": "imageprocess"
    },
    {
      "source": "arduino-edgeimpulse",
      "target": "arduinosensor"
    },
    {
      "source": "arduinodeploy",
      "target": "arduino-edgeimpulse"
    },
    {
      "source": "fft",
      "target": "edge-audio"
    },
    {
      "source": "edge-motion",
      "target": "arduinodeploy"
    },
    {
      "source": "edge-audio",
      "target": "edge-motion"
    },
    {
      "source": "anomaly_numpy",
      "target": "ml"
    },
    {
      "source": "wordvector",
      "target": "dl"
    },
    {
      "source": "wordvector",
      "target": "llm"
    },
    {
      "source": "chart",
      "target": "python"
    },
    {
      "source": "kmeans",
      "target": "ml"
    },
    {
      "source": "easyocr",
      "target": "imageprocess"
    },
    {
      "source": "crossentropy",
      "target": "dl"
    },
    {
      "source": "configuration",
      "target": "llm"
    },
    {
      "source": "configuration",
      "target": "temperature"
    },
    {
      "source": "fewshot",
      "target": "llm"
    },
    {
      "source": "evaluation",
      "target": "llm"
    },
    {
      "source": "ner",
      "target": "dl"
    },
    {
      "source": "finetuning",
      "target": "llm"
    },
    {
      "source": "finetuning",
      "target": "evaluation"
    },
    {
      "source": "faq",
      "target": "llm"
    },
    {
      "source": "temperature",
      "target": "configuration"
    },
    {
      "source": "quantization",
      "target": "finetuning"
    },
    {
      "source": "quantization",
      "target": "dl"
    },
    {
      "source": "quantization_2",
      "target": "quantization"
    },
    {
      "source": "poster",
      "target": "devfest"
    },
    {
      "source": "poster",
      "target": "python"
    },
    {
      "source": "devfest",
      "target": "reinvent"
    },
    {
      "source": "reinvent",
      "target": "reinvent_2"
    },
    {
      "source": "reinvent_2",
      "target": "reinvent"
    },
    {
      "source": "langchain_tools",
      "target": "agentic-architecture"
    },
    {
      "source": "arduinosensor",
      "target": "dl"
    },
    {
      "source": "deepdream",
      "target": "imageprocess"
    },
    {
      "source": "gnn_deep",
      "target": "dl"
    },
    {
      "source": "gnn_steps",
      "target": "gnn_deep"
    },
    {
      "source": "gnn_simple",
      "target": "gnn_steps"
    },
    {
      "source": "gnn_improved",
      "target": "gnn_simple"
    },
    {
      "source": "agentic-architecture",
      "target": "llm"
    },
    {
      "source": "dl",
      "target": "transformers"
    },
    {
      "source": "ml",
      "target": "python"
    },
    {
      "source": "ml",
      "target": "dl"
    },
    {
      "source": "python",
      "target": "ml"
    }
  ]
}