---
layout: page
title: Academic Studies
permalink: /Academic Studies/
---

Articles that I've Read: 
- Attention Is All You Need, Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, 
  A. N., Kaiser, L., Polosukhin, I. [link](arXivpreprintarXiv:1706.03762,2017)


Posts
- [AI Poses Doomsday Risks—But That Doesn’t Mean We Shouldn’t Talk About Present Harms Too](https://time.com/6303127/ai-future-danger-present-harms/)

        Notes: 
        "First, the public currently has little to no say over what models are built and how they are deployed.those most 
        affected by welfare systems have had little say in their automation. 
         
        Second, a strong auditing regime, where independent third-parties would scrutinize the practices and development 
        processes of AI labs, would help reduce risks overall. 
  
        Third, we should require meaningful human oversight of critical AI decisions, and avoid very-high-risk use cases such 
        as lethal autonomous weapons. 
  
        Fourth, we should rebalance the  funding going into AIs and urge companies to spend less on making them smarter 
        while increasing funding into making them safer, more transparent, and studying their social impacts. "

  - [Thousands of AI Authors on the Future of AI](https://arxiv.org/abs/2401.02843)

        Notes: 
        2,778 researchers were surveyed who had published in top-tier artificial
        intelligence (AI) venues, asking for their predictions on the pace of AI progress and the nature and
        impacts of advanced AI systems. 
      
        "We defined High-Level Machine Intelligence (HLMI) thus:
        High-level machine intelligence (HLMI) is achieved when unaided machines can accomplish every
        task better and more cheaply than human workers. Ignore aspects of tasks for which being a human
        is intrinsically advantageous, e.g. being accepted as a jury member. Think feasibility, not adoption"
      
        “Full Automation of Labor” (FAOL).
      
        
        HLMI and FAOL are quite similar: FAOL asks about the automation of all occupations; HLMI asks about the
        feasible automation of all tasks.Predictions for a 50% chance of the arrival of FAOL are consistently more than sixty 
        years later than those for a 50%
        chance of the arrival of HLMI. 
        
        What causes AI progress?
        1) researcher effort, 2) decline
        in cost of computation, 3) effort put into increasing the size and availability of training datasets, 4) funding, and 5)
        progress in AI algorithms.
        
        Explainability:
        Most respondents considered it unlikely that users of AI systems in 2028 will be able to know the true
        reasons for the AI systems’ choices, with only 20% giving it better than even odds.
        
        Amount of concern potential scenarios deserve, organized from most to least extreme concern:
        Less concern on "Near full automation of labor makes people struggle to find meaning in their lives" and most concern on 
        AI makes it easy to spread false information. 
        
        How much should society prioritize AI safety research, relative to how much it
        is currently prioritized?  70% of respondents thought AI safety research should be prioritized more than it currently is.
        Examples of AI safety research might include:
        • Improving the human-interpretability of machine learning algorithms for the purpose of improving the safety and robustness of AI systems, not focused on improving AI capabilities
        • Research on long-term existential risks from AI systems
        • AI-specific formal verification research
        • Policy research about how to maximize the public benefits of AI
        • Developing methodologies to identify, measure, and mitigate biases in AI models to ensure fair
        and ethical decision-making



