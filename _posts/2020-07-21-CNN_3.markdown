---
layout: post
title:  "Style"
date:   2020-10-19
categories: General
---

# CNN


```python
from google.colab import drive
drive.mount('/content/drive')
```

    Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).



```python
import os
os.chdir("/content/drive/My Drive/Style_Transfer/notebook_ims")
!ls
```

    style_tx_cat.png  vgg19_convlayers.png



```python
# import resources
%matplotlib inline

from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt
import numpy as np

import torch
import torch.optim as optim
import requests
from torchvision import transforms, models
```

Load VGG features, not classification part


```python
# get the "features" portion of VGG19 (we will not need the "classifier" portion)
# all convolutional and pooling layers are in features 
vgg = models.vgg19(pretrained=True).features

# freeze all VGG parameters since we're only optimizing the target image
for param in vgg.parameters():
    param.requires_grad_(False)
```

    Downloading: "https://download.pytorch.org/models/vgg19-dcbb9e9d.pth" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth



    HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))


    



```python
# move the model to GPU, if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vgg.to(device)

'''
(0) -> conv1_1
(1) -> conv1_2 (second in the first stack)
Max Pooling Layer
(5) -> conv2_1 (first in second stack)

'''
```




    '\n(0) -> conv1_1\n(1) -> conv1_2 (second in the first stack)\nMax Pooling Layer\n(5) -> conv2_1 (first in second stack)\n\n'



Load in Content and Style Images


```python
def load_image(img_path, max_size=400, shape=None):
    ''' Load in and transform an image, making sure the image
       is <= 400 pixels in the x-y dims.'''
    if "http" in img_path:
        response = requests.get(img_path)
        image = Image.open(BytesIO(response.content)).convert('RGB')
    else:
        image = Image.open(img_path).convert('RGB')
    
    # large images will slow down processing
    if max(image.size) > max_size:
        size = max_size
    else:
        size = max(image.size)
    
    if shape is not None:
        size = shape
        
    in_transform = transforms.Compose([
                        transforms.Resize(size),
                        transforms.ToTensor(),
                        transforms.Normalize((0.485, 0.456, 0.406), 
                                             (0.229, 0.224, 0.225))])

    # discard the transparent, alpha channel (that's the :3) and add the batch dimension
    image = in_transform(image)[:3,:,:].unsqueeze(0)
    
    return image
```

Load images by file name and forcing the style image to be the same size as the content image.


```python
# load in content and style image
content = load_image('/content/drive/My Drive/Style_Transfer/images/octopus.jpg').to(device)
#content = load_image('/content/drive/My Drive/Style_Transfer/images/mother.jpeg').to(device)
# Resize style to match content, makes code easier
style = load_image('/content/drive/My Drive/Style_Transfer/images/hockney.jpg', 
                   shape=content.shape[-2:]).to(device)

content.shape
```




    torch.Size([1, 3, 400, 592])




```python
# helper function for un-normalizing an image 
# and converting it from a Tensor image to a NumPy image for display
def im_convert(tensor):
    """ Display a tensor as an image. """
    
    image = tensor.to("cpu").clone().detach()
    image = image.numpy().squeeze()
    image = image.transpose(1,2,0)
    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
    image = image.clip(0, 1)

    return image
```


```python
# display the images
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
# content and style ims side-by-side
ax1.imshow(im_convert(content))
ax2.imshow(im_convert(style))
```




    <matplotlib.image.AxesImage at 0x7fa1d6a85588>




![png](output_11_1.png)


Content and Style Features

TODO: complete the mapping of layer names to the names found in the paper for the _content representation_ and the _style representation_.


```python
def get_features(image, model, layers=None):
    """ Run an image forward through a model and get the features for 
        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)
    """
    
    ## TODO: Complete mapping layer names of PyTorch's VGGNet to names from the paper
    ## Need the layers for the content and style representations of an image
    if layers is None:
        layers = {'0': 'conv1_1',
                  '5': 'conv2_1', 
                  '10': 'conv3_1',   
                  '19': 'conv4_1',
                  '21': 'conv4_2', # content representation
                  '28': 'conv5_1', 
                  }
        